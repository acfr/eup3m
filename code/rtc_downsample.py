# Implement entropy-based downsampling routine
#
# Rio Tinto Centre
# Faculty of Engineering
# The University of Sydney
#
# SPDX-FileCopyrightText: 2024 Alexander Lowe <alexander.lowe@sydney.edu.au>
# SPDX-License-Identifier: BSD-3-Clause
#------------------------------------------------------------------------------------------------


from queue import Empty

import numpy as np
import multiprocessing as mp
from scipy.spatial.ckdtree import cKDTree
import scipy.stats

NUM_PROCESSES = mp.cpu_count()

def downsample(xyz, v, n, rand_state=0, wait_period=2):
    '''
    Perform optimal downsampling based on KL divergence.
    Select n rows of [xyz|v] that best preserve the distribution of the full dataset.
    :param xyz: points or spatial coordinates, numpy.array with shape=(m,3)
    :param v: corresponding values, numpy.array with shape=(m,)
    :param n: number of points after downsampling, n <= m
    :param rand_state: optional random state
    :param wait_period: sets the msg_queue.get() block duration
    '''
    if n == 0:
        return np.array([], dtype=int)

    if n >= len(xyz):
        return np.arange(len(xyz))

    best_queue = mp.Queue()
    msg_queue = mp.Queue()
    n_iterations = xyz.shape[0]

    lower_bound = min(v)
    upper_bound = max(v)
    xyz_tree = cKDTree(xyz)

    complete_pmf = _probability_mass_function(v, lower_bound, upper_bound)

    #Specify arguments list and perform subsampling using multiprocessing
    args = (xyz, n, xyz_tree, v, complete_pmf, lower_bound, upper_bound,
            int(n_iterations / NUM_PROCESSES), rand_state, best_queue, msg_queue)

    subsampling_jobs = None
    if NUM_PROCESSES > 1:
        subsampling_jobs = [mp.Process(target=_downsample_worker,
                            args=(args + (i,))) for i in range(NUM_PROCESSES)]
        for job in subsampling_jobs:
            job.start()
    else:
        _downsample_worker(*(args + (0,)))

    #Handle log messages generated by subsampling processes
    _subsample_message_helper(msg_queue)

    best_indices = []
    best_KLs = []

    for i in range(NUM_PROCESSES):
        #A small timeout is used since the `best_queue` is fed at the very
        #end of _downsample_worker, it is possible that the `best_queue`
        #is NOT yet populated while the main process is preoccupied with
        #downsampling and pushing messages into the `msg_queue`
        idx, kl = best_queue.get(timeout=wait_period)
        best_indices.append(idx)
        best_KLs.append(kl)

    min_KL = min(best_KLs)
    min_KL_idx = best_indices[best_KLs.index(min_KL)]

    #The threads have to be joined after the queues have been fully consumed
    if subsampling_jobs:
        for job in subsampling_jobs:
            job.join()

    #Capture the best result
    mask = np.zeros(len(xyz), dtype=int)
    mask[min_KL_idx] = 1
    print('Downsampling results: min d_KL={}, selected=[{}]'.format(
           min_KL, ','.join(['%d' % x for x in mask])))
    return min_KL_idx


def _downsample_worker(xyz, npts, xyz_tree, v, complete_pmf,
                       lower_bound, upper_bound, n_iterations, rand_state,
                       best_queue, msg_queue, process_idx):
    '''
    A downsample factory that executes n_iterations of randomised sampling
    in separate processes. The objective is to find a downsample with the
    smallest KL divergence relative to the distribution of the full dataset.
    :param xyz: points or spatial coordinates, numpy.array with shape=(m,3)
    :param npts: number of points after downsampling, npts <= m
    :param xyz_tree: a scipy.spatial.ckdtree used for finding nearest points
    :param v: values corresponding to xyz, numpy.array with shape=(m,)
    :param complete_pmf: reference distribution for the full dataset using all values
    :param lower_bound: a lower bound for `v`
    :param upper_bound: an upper bound for `v`
    :param n_iterations: number of random iterations
    :param rand_state: random state used for setting up the random number generator
    :param best_queue: multiprocessing.Queue used for keeping the best subsampling
                       results. Each element is a tuple of the lowest KL value and
                       corresponding subsample indices
    :param msg_queue: multiprocessing.Queue used for passing message between
                      downsampling processes and the message handling process
    :param process_idx: the index of the current process
    '''
    random_number_generator = np.random.RandomState(rand_state + process_idx)

    f = lambda x: np.array(np.percentile(x, np.linspace(0, 100, 11)))
    target = f(values)

    best_indices = None
    best_KL = np.inf
    #An upper bound of infinity is preferred over 1 [even though the risk
    #of misbehaving is low] as KLD is unbounded and the default base is "e"

    reporting_period = np.inf #max(int(0.1 * n_iterations), 100)

    for n in range(0, n_iterations):
        #- generate a random subsample
        #- compute pmf for the subsample
        #- compute the relative entropy (this is equivalent to the KL divergence which
        #  measures the discrepancies between the sampled and complete data distribution)
        idx = _downsample(xyz, npts, xyz_tree, random_number_generator)
        sample_pmf = _probability_mass_function(values[idx], lower_bound, upper_bound)
        kl = scipy.stats.entropy(sample_pmf, complete_pmf)
        if kl < best_KL:
            best_KL = kl
            best_indices = idx
        if n > 0 and not n % reporting_period:
            msg_queue.put('Sub-sampling completed iteration {} of {} on process {}'
                             .format(n, n_iterations, process_idx))

    msg_queue.put('Sub-sampling completed {} iterations on process {}. Best KL was {}.'
                     .format(n_iterations, process_idx, best_KL))
    msg_queue.put('Done') #only used for terminating the process
    best_queue.put((best_indices, best_KL))


def _probability_mass_function(data, lower_bound, upper_bound):
    '''
    Compute the probability mass function for the data.
    :param data: input values, numpy.array with shape=(m,)
    :param lower_bound: a lower bound for the values
    :param upper_bound: an upper bound for the values
    '''
    histogram, bins = np.histogram(data, bins=100, range=(lower_bound, upper_bound), density=True)

    #Normalisation ensures histogram values sum to 1
    return histogram * np.diff(bins)


def _subsample_message_helper(msg_queue, msg_wait=300):
    '''
    Write messages from different downsampling processes into the log file.
    This method does not return until all worker threads are complete.
    This ensures the best_queue gets fully populated by the time we try to read it.
    :param msg_queue: multiprocessing.Queue used for passing message between
                      downsampling processes and the message handling process
    :param msg_wait: number of seconds for asynchronous get. Default value of 300s is adequate
                     for our experiments where the max number of iterations is about 5000.
    '''
    processes_completed = 0
    while processes_completed < NUM_PROCESSES:
        #added next line to eliminate "UnboundLocalError: local variable 'message' referenced before assignment"
        message = ''
        try:
            message = msg_queue.get(timeout=msg_wait)
        except Empty:
            pass
        if message == 'Done':
            processes_completed += 1
        else:
            print(message)


def _downsample(xyz, npts, xyz_tree, random_number_generator):
    '''
    Perform actual downsampling operation following a cluster-based approach.
    :param xyz: points or spatial coordinates, numpy.array with shape=(m,3)
    :param npts: number of points after downsampling, npts <= m
    :param xyz_tree: a scipy.spatial.ckdtree used for finding nearest points
    :param random_number_generator: pseudo-random number generator
    :return samples: list of subsampled indices
    '''
    if xyz.shape[0] <= npts:
        return np.arange(xyz.shape[0], dtype=int)

    patch_size = 1 + int(round(np.sqrt(npts)))
    samples = np.empty(shape=0, dtype=int)

    while len(samples) < npts:
        patches_required = int(np.ceil((npts - len(samples)) / patch_size))
        if len(samples) > 0:
            patches_required *= 2

        ix = random_number_generator.randint(0, xyz.shape[0] - 1, size=patches_required)
        d, ix = xyz_tree.query(xyz[ix, :3], patch_size)
        samples = np.unique(np.concatenate((samples, ix.flatten())))

    if len(samples) > npts:
        random_number_generator.shuffle(samples)
        samples = samples[:npts]

    return samples
