{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Uncertainty and Predictive Performance of Probabilistic Models\n",
    "\n",
    "<b>Raymond Leung</b><br>\n",
    "Rio Tinto Centre, Faculty of Engineering<br>\n",
    "The University of Sydney, 2024\n",
    "\n",
    "`SPDX-FileCopyrightText: 2024 Raymond Leung and Alexander Lowe <raymond.leung@sydney.edu.au>`<br>\n",
    "`SPDX-License-Identifier: `[`BSD-3-Clause`](https://opensource.org/license/BSD-3-Clause)<br>\n",
    "\n",
    "### Examining Geostatistical Properties across Domains and Inference Periods\n",
    "\n",
    "#### <font color=\"#cc0066\">Pre-requisite</font>\n",
    "- Having completed the experiments for all geological domains (gD) and inference periods (mA) typically by running the bash script `run_experiments.sh` on a compute node.\n",
    "- Alternatively, use the cached results from the `../archive/` directory.\n",
    "\n",
    "#### Available statistics\n",
    "- Uncorrelated error: `RMSE`\n",
    "- Histogram differences (model, groundtruth): `h(psChi2)`, `h(JS)`, `h(IOU)`, `h(EM)`, `h(rank)`\n",
    "- Variogram-based spatial fidelity: `Variogram Ratios`, `Spatial Fidelity`\n",
    "- S-scores and Uncertainty-based measures: `|s|_L`, `|s|_U`, `Consensus`, `Accuracy(.05)`, `Precision`, `Goodness`, `Tightness`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Python modules\n",
    "import ast\n",
    "import copy\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skgstat as skg\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "import string\n",
    "import warnings\n",
    "\n",
    "sys.path.append(os.getcwd().replace('notebook', 'code'))\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from collections import OrderedDict\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy.stats import ttest_rel, ttest_ind\n",
    "from rtc_evaluation_metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "# CONFIGURE\n",
    "\n",
    "# Inference mode\n",
    "inference_mode = 'future-bench-prediction'\n",
    "\n",
    "# Specify directories\n",
    "base_dir = os.getcwd()\n",
    "code_dir = base_dir.replace('notebook', 'code') #$experiment/code/\n",
    "data_dir = base_dir.replace('notebook', 'data') #$experiment/data/\n",
    "# result_dir may point to your $experiment/results/ instead\n",
    "result_dir = re.sub('notebook', f\"archive/{inference_mode}\", base_dir)\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "# Domain properties\n",
    "inference_mode_prefix = \"insitu_\" if \"in-situ-regression\" in inference_mode else \"\"\n",
    "gdp_csv = os.path.join(data_dir, f\"{inference_mode_prefix}domain_samples_summary.csv\")\n",
    "df_gdp = pd.read_csv(gdp_csv)\n",
    "df_gdp = df_gdp.rename(columns = {'# mA': 'mA'})\n",
    "\n",
    "# Blocks data\n",
    "blocks_csv = 'blocks_to_estimate_tagged.csv'\n",
    "if \"in-situ-regression\" in inference_mode:\n",
    "    blocks_csv = 'blocks_insitu_tagged.csv'\n",
    "\n",
    "def data_available_for(inference_period, domain_id):\n",
    "    return not df_gdp.query('@inference_period == mA and @domain_id == domain').empty\n",
    "\n",
    "inference_periods = df_gdp['mA'].values\n",
    "domain_ids = df_gdp['domain'].values\n",
    "n_blastholes = df_gdp['n_blastholes'].values\n",
    "n_inference_pts = df_gdp['n_inference_pts'].values\n",
    "uniq_inference_periods = np.unique(inference_periods)\n",
    "uniq_domain_ids = np.unique(domain_ids)\n",
    "\n",
    "# Performance statistics\n",
    "example_stats_csv = os.path.join(result_dir, 'learning_rotated/04_05_06/analysis-2310.csv')\n",
    "df_example = pd.read_csv(example_stats_csv, index_col=0, header=0)\n",
    "model_names = list(df_example.index)\n",
    "two_powers = [int(m.replace('GP_CRF_from_','')) for m in model_names if 'CRF_from' in m]\n",
    "n_exponents = int(np.log2(max(two_powers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some definitions for computing semi-variograms\n",
    "specs = {}\n",
    "variogram_use_nugget = specs.get('variogram:use_nugget', False)\n",
    "variogram_max_lag = specs.get('variogram:max_lag', 250.0)\n",
    "variogram_num_lag = specs.get('variogram:num_lag', 45)\n",
    "variogram_required_samples = specs.get('variogram:required_samples', 30)\n",
    "variogram_model = specs.get('kriging:covariance_fn', 'matern')\n",
    "fixed_nu = specs.get('kriging:matern_smoothness', None)\n",
    "max_range = 2 * variogram_max_lag\n",
    "constraints = None\n",
    "if fixed_nu:\n",
    "    constraints = ([0., 0., fixed_nu - 0.0001, 0.],\n",
    "                   [max_range, max_var, fixed_nu + 0.0001, 0.5 * max_var])\n",
    "    if variogram_use_nugget is False:\n",
    "        constraints = tuple([cs[:-1] for cs in constraints])\n",
    "make_variogram = lambda x, y : skg.Variogram(\n",
    "                     coordinates=x,\n",
    "                     values=y,\n",
    "                     estimator='matheron',\n",
    "                     model=variogram_model,\n",
    "                     use_nugget=variogram_use_nugget,\n",
    "                     maxlag=max_range,\n",
    "                     n_lags=variogram_num_lag,\n",
    "                     fit_bounds=constraints)\n",
    "\n",
    "# Extend results to reference model(s)\n",
    "def compute_results_for_reference_models(attribute, mA, domain_id):\n",
    "    # Specify scope\n",
    "    mA_mB_mC = '%02d_%02d_%02d' % (mA, mA+1, mA+2)\n",
    "    data_path = os.path.join(data_dir, mA_mB_mC)\n",
    "    df_k = pd.read_csv(f\"{data_path}/{blocks_csv}\")\n",
    "    df_domain_infer = pd.DataFrame(df_k[df_k['domain'] == domain_id])\n",
    "    ground_truth = mu_0 = df_domain_infer['cu_bh_nn'].values\n",
    "    if df_domain_infer.empty:\n",
    "        return np.nan, np.nan    \n",
    "    # Retrieve grades for long-range and bench-above models\n",
    "    gt_long_range = df_domain_infer['cu_50'].values\n",
    "    stats = dict()\n",
    "    # Compute relevant statistic\n",
    "    if attribute in ['RMSE']:\n",
    "        for k, y in zip(['RTK_LongRangeModel'],\n",
    "                        [gt_long_range]):\n",
    "            retain = np.where(np.isfinite(y))[0]\n",
    "            stats[k] = compute_root_mean_squared_error(mu_0[retain], y[retain])\n",
    "    elif 'h(' in attribute and attribute != 'h(rank)':\n",
    "        short_names = ['h(psChi2)', 'h(JS)', 'h(IOU)', 'h(EM)', 'h(rank)']\n",
    "        long_names = ['psym-chi-square', 'jensen-shannon', 'ruzicka', 'wasserstein', None]\n",
    "        name_of = dict(zip(short_names, long_names))\n",
    "        for k, y in zip(['RTK_LongRangeModel'],\n",
    "                        [gt_long_range]):\n",
    "            retain = np.where(np.isfinite(y))[0]\n",
    "            stats[k] = compute_histogram_statistics(mu_0[retain], y[retain])[name_of[attribute]]\n",
    "    elif attribute == 'Variogram Ratios':\n",
    "        x = df_domain_infer[['X','Y','Z']].values\n",
    "        vgram = OrderedDict()\n",
    "        names = ['GroundTruth(blocks)', 'RTK_LongRangeModel']\n",
    "        grades = [mu_0, gt_long_range]\n",
    "        if len(ground_truth) >= variogram_required_samples:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings('ignore')\n",
    "                try:\n",
    "                    for k, y in zip(names, grades):\n",
    "                        retain = np.where(np.isfinite(y))[0]\n",
    "                        vgram[k] = make_variogram(x[retain], y[retain])\n",
    "                    # Stack the variograms\n",
    "                    v = np.vstack([vgram[k].experimental for k in names])\n",
    "                    v = v[:, np.isfinite(np.sum(v, axis=0))]\n",
    "                    for row, k in zip([1,2], names[1:]): #only compute 50th percentile\n",
    "                        ratios = v[row] / v[0]\n",
    "                        stats[k] = np.median(ratios)\n",
    "                except Exception:\n",
    "                    pass # Ignore \"All-NaN slice encountered\" or \"All input values are the same\"\n",
    "    return stats.get('RTK_LongRangeModel', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group models by logical partition [SK_like | OK_like | GP(G)_like | GP(L)_like]\n",
    "j, k = 8, n_exponents\n",
    "ordered_model_names = model_names[0:2] + model_names[j:j+k] \\\n",
    "                    + model_names[2:4] + model_names[j+k:j+2*k] \\\n",
    "                    + model_names[6:8] + model_names[j+3*k:j+4*k] \\\n",
    "                    + model_names[4:6] + model_names[j+2*k:j+3*k]\n",
    "\n",
    "n_rows = len(df_example)\n",
    "n_columns = len(uniq_inference_periods) * len(uniq_domain_ids)\n",
    "n_gd = len(uniq_domain_ids)\n",
    "\n",
    "# Map \"<L><G><P><R>\" domain IDs to alphabets\n",
    "domain_symbols = dict(zip(uniq_domain_ids, list(string.ascii_uppercase[:len(uniq_domain_ids)])))\n",
    "\n",
    "'''\n",
    "Implementation of a custom image visualisation function\n",
    "'''\n",
    "def imshow_matrix_common(rspace, attribute, colourmap, cfg={}):\n",
    "    \"\"\"\n",
    "    @brief  Render an attribute matrix M as a colour image,\n",
    "            M.shape=(rows,cols) where rows = #model, cols = (#domain * #inference_period)\n",
    "    @detail Options available via cfg dict\n",
    "     - 'group_by' (str) ['domain,inference', 'inference,domain']\n",
    "            Outer and inner FOR loops w.r.t. x axis, whether to have\n",
    "            geological domain or inference period as the major category\n",
    "     - 'include_reference_models' (bool)\n",
    "            Add extra rows for RTK_LongRangeModel and RTK_BenchAboveData\n",
    "     - 'v_min', 'v_max' (float)\n",
    "            Set value limits for the data and colour bar\n",
    "     - 'hide_unmodelled_columns' (bool)\n",
    "            Remove (domain,period) columns where groundtruth is unavailable (modelling is not done)\n",
    "     - 'decorate_nans' (bool)\n",
    "            Use marker to represent NaNs to distinguish them from high-intensity values\n",
    "     - 'fontsize' (int)\n",
    "            Font size applicable to the x-axis, y-axis and the graph title\n",
    "     - 'title' (str)\n",
    "            Heading shown at the top of the figure\n",
    "     - 'colorbar_title' (str)\n",
    "            Optional label for the colour bar (default: \"\")\n",
    "     - 'compute_ratio' (str) variable abbreviated as k\n",
    "            If k is non-empty, the result for the current attribute will be divided by\n",
    "            a previously calculated attribute matrix, cfg[k]['M'], and rendered as an image.\n",
    "     - 'skip_plot' (bool) default: False\n",
    "            Enabled by user with the intent of obtaining the attribute matrix only\n",
    "     - 'return_config_only' (bool) default: False\n",
    "            Used in the first-pass to retrieve the updated config dictionary populated\n",
    "            with f\"results:{rspace}_{attribute}\" instead of the matrix M when 'skip_plot'\n",
    "            is True. The intent is to contrast the results with rspace='learning_rotated'\n",
    "            and rspace='not_rotated' and plotting their ratios in a subsequent call with\n",
    "            'compute_ratio' set to f\"{rspace}_{attribute}\"\n",
    "    \"\"\"\n",
    "    col = 0\n",
    "    n_ip = len(uniq_inference_periods)\n",
    "    n_gd = len(uniq_domain_ids)\n",
    "    include_reference_models = cfg.get('include_reference_models', False)\n",
    "    non_probabilistic_models = ['LongRangeModel']\n",
    "    #non_probabilistic_models = ['LongRangeModel', 'BenchAboveModel']\n",
    "    total_rows = n_rows + len(non_probabilistic_models) if include_reference_models else n_rows\n",
    "    M = np.nan * np.ones((total_rows, n_columns))\n",
    "    group_by = cfg.get('group_by', 'domain,inference')\n",
    "    v_min = cfg.get('v_min', -np.inf)\n",
    "    v_max = cfg.get('v_max', np.inf)\n",
    "    eps = cfg.get('eps', 1e-6)\n",
    "    modelling_required = []\n",
    "\n",
    "    if group_by == 'domain,inference':\n",
    "        outer_vars = uniq_domain_ids\n",
    "        inner_vars = uniq_inference_periods\n",
    "    else:\n",
    "        outer_vars = uniq_inference_periods\n",
    "        inner_vars = uniq_domain_ids\n",
    "\n",
    "    for vo in outer_vars:\n",
    "\n",
    "        for vi in inner_vars:\n",
    "            gd = vo if group_by == 'domain,inference' else vi\n",
    "            mA = vi if group_by == 'domain,inference' else vo\n",
    "            modelling_required.append(data_available_for(mA, gd))\n",
    "            mA_mB_mC = f\"%02d_%02d_%02d\" % (mA, mA+1, mA+2)\n",
    "            analysis_csv = os.path.join(result_dir, rspace, f\"{mA_mB_mC}\", f\"analysis-{gd}.csv\")\n",
    "            try:\n",
    "                df = pd.read_csv(analysis_csv, index_col=0, header=0)\n",
    "                df.rename(columns={'Likelihood': 'Consensus'}, inplace=True)",
    "                M[:n_rows, col] = np.minimum(np.maximum(\n",
    "                                  [df.loc[m, attribute] for m in ordered_model_names], v_min), v_max)\n",
    "                if include_reference_models:\n",
    "                    local_attribute = 'Variogram Ratios' if attribute == 'Spatial Fidelity' else attribute\n",
    "                    values = compute_results_for_reference_models(local_attribute, mA, gd)\n",
    "                    if attribute == 'Spatial Fidelity':\n",
    "                        values = np.sqrt(1 - np.abs(np.minimum(values, 2) - 1))\n",
    "                    M[n_rows:, col] = np.minimum(np.maximum(values, v_min), v_max)\n",
    "            except:\n",
    "                pass\n",
    "            col += 1\n",
    "\n",
    "    all_ordered_model_names = ordered_model_names + (non_probabilistic_models if include_reference_models else [])\n",
    "    all_model_names_for_display = [x + ('*' if x in non_probabilistic_models else '') for x in all_ordered_model_names]\n",
    "\n",
    "    # (Option) Hide columns where there is no groundtruth data (no modelling is done)\n",
    "    frac_columns_removed = 0.0\n",
    "    if cfg.get('hide_unmodelled_columns', False):\n",
    "        # - truncate matrix by discarding columns with no data, less distraction for the viewer\n",
    "        modelling_required = np.array(modelling_required, dtype=bool)\n",
    "        frac_columns_removed = sum(modelling_required == False) / n_columns\n",
    "        M = M[:, modelling_required]\n",
    "\n",
    "    # Make data available to user via the cfg dictionary\n",
    "    cfg[f\"results:{rspace}_{attribute}\"] = {'ordered_model_names': all_ordered_model_names, 'M': M}\n",
    "\n",
    "    # (Option) Compute ratio using a previously calculated attribute matrix\n",
    "    if cfg.get('compute_ratio', False):\n",
    "        k = cfg['compute_ratio']\n",
    "        M /= (cfg[f\"results:{k}\"]['M'] + eps)\n",
    "        M = np.minimum(np.maximum(M, v_min), v_max)\n",
    "        cfg['skip_plot'] = False # automatically disabled\n",
    "\n",
    "    if cfg.get('skip_plot', False):\n",
    "        return cfg if cfg.get('return_config_only', False) else M\n",
    "\n",
    "    fig = plt.figure(figsize=(15,10)) #width, height\n",
    "    im = plt.imshow(M, cmap=colourmap, aspect=2)\n",
    "\n",
    "    # (Option) Mark NaN locations with a distinctive marker to distinguish from bright intensity values\n",
    "    if cfg.get('decorate_nans', False):\n",
    "        pos = np.where(np.isnan(M))\n",
    "        plt.scatter(pos[1], pos[0], s=4, c='k', marker='o')\n",
    "\n",
    "    # Horizontal dividers\n",
    "    for i in range(int(M.shape[0] / (2 + n_exponents))):\n",
    "        plt.plot([-0.5, (M.shape[1]-1)+0.5], [(i+1)*(2+n_exponents)-0.5]*2, 'w')\n",
    "\n",
    "    ax = plt.gca()\n",
    "    _ = ax.set_xticks(np.arange(M.shape[1]))\n",
    "    _ = ax.set_xticklabels([]) #leave xtick labels as blank to avoid overcrowding\n",
    "    _ = ax.set_yticks(np.arange(M.shape[0]))\n",
    "    _ = ax.set_yticklabels(all_model_names_for_display, fontsize=8)\n",
    "    plt.text(0.0, 1.005,'* denotes non-probabilistic models', fontsize=8,\n",
    "             horizontalalignment='left', verticalalignment='bottom', transform=ax.transAxes)\n",
    "\n",
    "    # Zigzag placement of primary x-labels\n",
    "    if group_by == 'domain,inference':\n",
    "        # inner-axis describes inference\n",
    "        complete_inner_labels = list(uniq_inference_periods) * len(uniq_domain_ids)\n",
    "        x1_spacing_odd = '     '\n",
    "    else:\n",
    "        # inner-axis describes domain\n",
    "        complete_inner_labels = list(uniq_domain_ids) * len(uniq_inference_periods)\n",
    "        x1_spacing_odd = '         '\n",
    "\n",
    "    # - Make adjustment if columns have been discarded\n",
    "    if cfg.get('hide_unmodelled_columns', False):\n",
    "        complete_inner_labels = [x for i,x in enumerate(complete_inner_labels) if modelling_required[i]]\n",
    "    \n",
    "    complete_inner_ticks = np.arange(M.shape[1])\n",
    "    desc_x1_even = complete_inner_labels[::2]\n",
    "    desc_x1_odd = [f\"{x}{x1_spacing_odd}\" for x in complete_inner_labels[1::2]]\n",
    "    ticks_x1_even = complete_inner_ticks[::2]\n",
    "    ticks_x1_odd = complete_inner_ticks[1::2]\n",
    "    prim_even = ax.secondary_xaxis(location=0)\n",
    "    prim_even.set_xticks(ticks_x1_even, labels=desc_x1_even, ha='center', fontsize=8, rotation='vertical')\n",
    "    prim_even.tick_params('x', length=0.05)\n",
    "    prim_odd = ax.secondary_xaxis(location=0)\n",
    "    prim_odd.set_xticks(ticks_x1_odd, labels=desc_x1_odd, ha='center', fontsize=8, rotation='vertical')\n",
    "    prim_odd.tick_params('x', length=0.05)\n",
    "\n",
    "    # Nested secondary x-labels (outer-axis)\n",
    "    x2_vert_spacing = '\\n\\n\\n' if group_by == 'domain,inference' else '\\n\\n\\n\\n\\n'\n",
    "    xlabel_vert_spacing = '\\n\\n\\n\\n' if group_by == 'domain,inference' else '\\n\\n\\n\\n\\n'\n",
    "    desc_x2 = [f\"{x2_vert_spacing}{x}\" for x in outer_vars]\n",
    "    if group_by == 'domain,inference':\n",
    "        ticks_x2 = [n_ip/2. + i * n_ip for i in range(len(uniq_domain_ids))]\n",
    "        ticks_line_x2 = np.array([0] + [(i+1) * n_ip for i in range(len(uniq_domain_ids))]) - 0.5\n",
    "        outer_axis_y_offset = -0.07 * (1 - frac_columns_removed)\n",
    "        x_desc = '(Inner) Inference Period / (Outer) Domain Label'\n",
    "    else:\n",
    "        ticks_x2 = [n_gd/2. + i * n_gd for i in range(len(uniq_inference_periods))]\n",
    "        ticks_line_x2 = np.array([0] + [(i+1) * n_gd for i in range(len(uniq_inference_periods))]) - 0.5\n",
    "        outer_axis_y_offset = -0.12 * (1 - frac_columns_removed)\n",
    "        x_desc = '(Inner) Domain Label / (Outer) Inference Period'\n",
    "\n",
    "    # - Make adjustment to horizontal spacing if columns have been discarded\n",
    "    if cfg.get('hide_unmodelled_columns', False):\n",
    "        discard = 1 - np.array(modelling_required, dtype=int)\n",
    "        xspace_reduction = np.array([sum(discard[:int(x+0.5)]) for x in ticks_line_x2[1:]])\n",
    "        ticks_line_x2 -= np.hstack([[0], xspace_reduction])\n",
    "        ticks_x2 = [(left + right)/2. for left, right in zip(ticks_line_x2[:-1], ticks_line_x2[1:])]\n",
    "\n",
    "    sec = ax.secondary_xaxis(location=0)\n",
    "    sec.set_xticks(ticks_x2, labels=desc_x2, ha='center')\n",
    "    sec.tick_params('x', length=0)\n",
    "\n",
    "    # lines between outer labels\n",
    "    sec2 = ax.secondary_xaxis(location=outer_axis_y_offset)\n",
    "    sec2.set_xticks(ticks_line_x2, labels=[])\n",
    "    sec2.tick_params('x', length=18, width=1)\n",
    "\n",
    "    fs = cfg.get('fontsize', 12)\n",
    "    plt.xlabel(f\"{xlabel_vert_spacing}{x_desc}\", fontsize=fs)\n",
    "    plt.ylabel('Probabilistic Model', fontsize=fs)\n",
    "    plt.title(cfg.get('title', ''), y=1.01, fontsize=fs)\n",
    "    plt.colorbar(im, fraction=0.025, pad=0.02, label=cfg.get('colorbar_title',''))\n",
    "\n",
    "def imshow_matrix_group_by_inference_domain(rspace, attribute, colourmap, cfg={}):\n",
    "    cfg['group_by'] = 'inference,domain'\n",
    "    imshow_matrix_common(rspace, attribute, colourmap, cfg)\n",
    "\n",
    "def imshow_matrix_group_by_domain_inference(rspace, attribute, colourmap, cfg={}):\n",
    "    #_cfg = copy.deepcopy(cfg)\n",
    "    cfg['group_by'] = 'domain,inference'\n",
    "    imshow_matrix_common(rspace, attribute, colourmap, cfg)\n",
    "\n",
    "def create_inverted_colormap(gamma=1, levels=256, monochrome=False, lscm_upper=None, lscm_lower=None,\n",
    "                             flip_upper=False, flip_lower=False):\n",
    "    \"\"\"\n",
    "    Combine a symmetric blue/red colour scale to cover range [-1,1] with darker shades near zero\n",
    "    :param gamma: nonlinearity (< 1 to expand, > 1 to compress small values nearest to zero)\n",
    "    :param levels: discretisation (number of colours in palette)\n",
    "    :param monochrome: use only half the palette nominally associated with negative values\n",
    "    :param lscm_upper: matplotlib.colors.LinearSegmentedColormap for positive values\n",
    "    :param lscm_lower: matplotlib.colors.LinearSegmentedColormap for negative values (or all values if monochrome=True)\n",
    "    :param flip_upper: reverse the intensity progression logic for lscm_upper\n",
    "    :param flip_lower: reverse the intensity progression logic for lscm_lower\n",
    "    \"\"\"\n",
    "    x = np.linspace(0, 1, levels//2)\n",
    "    xp = x**gamma\n",
    "    if lscm_upper is None:\n",
    "        lscm_upper = mpl.cm.Reds\n",
    "    if lscm_lower is None:\n",
    "        lscm_lower = mpl.cm.Blues\n",
    "    cmap_upper = lscm_upper(np.linspace(0, 1, levels//2))\n",
    "    cmap_lower = lscm_lower(np.linspace(0, 1, levels//2))\n",
    "    lower, upper = np.zeros((levels//2, 4)), np.zeros((levels//2, 4))\n",
    "    for rgba in range(4):\n",
    "        upper[:,rgba] = np.interp(xp, x, cmap_upper[:,rgba])[::-1]\n",
    "        lower[:,rgba] = np.interp(xp, x, cmap_lower[:,rgba])\n",
    "    if flip_upper:\n",
    "        upper = upper[::-1]\n",
    "    if flip_lower:\n",
    "        lower = lower[::-1]\n",
    "    if monochrome:\n",
    "        cmap = mpl.colors.ListedColormap(np.r_[lower])\n",
    "    else:\n",
    "        cmap = mpl.colors.ListedColormap(np.r_[lower, upper])\n",
    "    return cmap\n",
    "\n",
    "def fetch_property(rspace, attribute, cfg):\n",
    "    _cfg = copy.deepcopy(cfg)\n",
    "    _cfg['skip_plot'] = True\n",
    "    return imshow_matrix_common(rspace, attribute, plt.get_cmap('Blues_r'), _cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenient function for significance testing\n",
    "def t_test(x, y, which='dependent-t', ha='greater'):\n",
    "    '''\n",
    "    Wrapper for dependent and Welch's t-test\n",
    "    :param ha: alternative hypothesis type, one of ['less', 'greater', 'two-sided']\n",
    "    :return: (t-statistic, p-value, degree-of-freedom, CI.lower, CI.upper)\n",
    "    '''\n",
    "    if which == 'welch': #assumes independence/unequal variance\n",
    "        r_different = ttest_ind(x, y, nan_policy='omit', equal_var=False, alternative='two-sided')\n",
    "        r_inequality = ttest_ind(x, y, nan_policy='omit', equal_var=False, alternative=ha)\n",
    "    else: #employ dependent t-test\n",
    "        r_different = ttest_rel(x, y, nan_policy='omit', alternative='two-sided')\n",
    "        r_inequality = ttest_rel(x, y, nan_policy='omit', alternative=ha)\n",
    "    ci = r_different.confidence_interval(confidence_level=0.95)\n",
    "    r = r_inequality\n",
    "    rounded_pval = min(max(r.pvalue, 0.0001), 0.9999)\n",
    "    fmt = lambda x: np.round(x, 6)\n",
    "    return r.statistic, fmt(rounded_pval), r.df, fmt(ci.low), fmt(ci.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Cu grade histograms in different domains and inference periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "histogram_matrix_pfile = os.path.join(os.getcwd(), 'histogram_matrix.p')\n",
    "histogram_matrix_png = os.path.join(os.getcwd(), 'histogram_matrix.png')\n",
    "\n",
    "show_jsd_similarity = True\n",
    "\n",
    "n_bins = 40\n",
    "bin_edges = np.r_[-np.inf, np.linspace(0.05, 1.6, n_bins+1), np.inf]\n",
    "delta = np.diff(bin_edges[1:-1])\n",
    "centroids = np.mean(np.vstack([bin_edges[1:-2], bin_edges[2:-1]]), axis=0)\n",
    "bin_representative_values = np.r_[centroids[0]-delta[0], centroids, centroids[-1]+delta[-1]]\n",
    "w = np.median(np.diff(bin_representative_values))\n",
    "\n",
    "hist_training = {}\n",
    "hist_true_inference = {}\n",
    "training_groundtruth_hist_similarity = {}\n",
    "eps = 1e-9\n",
    "\n",
    "# load the plot if it was previously saved\n",
    "if os.path.exists(histogram_matrix_png):\n",
    "    img = Image(filename=histogram_matrix_png)\n",
    "    display(img)\n",
    "\n",
    "else: # create the plot from scratch\n",
    "    if os.path.exists(histogram_matrix_pfile):\n",
    "        with open(histogram_matrix_pfile, 'rb') as f:\n",
    "            (hist_training, hist_true_inference, training_groundtruth_hist_similarity) = pickle.load(f)\n",
    "    else:\n",
    "        for mA in uniq_inference_periods:\n",
    "            mA_mB_mC = f\"%02d_%02d_%02d\" % (mA, mA+1, mA+2)\n",
    "            for gd in uniq_domain_ids:\n",
    "                # read grade values from training data and groundtruth data for inferenced blocks\n",
    "                data_path = os.path.join(data_dir, f\"{mA_mB_mC}\")\n",
    "                df_bh = pd.read_csv(f\"{data_path}/blastholes_tagged.csv\")\n",
    "                df_bh = df_bh.rename(columns = {'EAST': 'X', 'NORTH': 'Y', 'RL': 'Z', 'PL_CU': 'V'})\n",
    "                df_domain_bh = df_bh[(df_bh['lookup_domain'] == gd) & np.isfinite(df_bh['V'].values)]\n",
    "                training_grades = df_domain_bh['V'].values\n",
    "                df_k = pd.read_csv(f\"{data_path}/{blocks_csv}\")\n",
    "                df_domain_k = pd.DataFrame(df_k[df_k['domain'] == gd])\n",
    "                inference_true_grades = df_domain_k['cu_bh_nn'].values\n",
    "                # compute and normalise histograms\n",
    "                counts_t, _ = np.histogram(training_grades, bin_edges)\n",
    "                counts_i, _ = np.histogram(inference_true_grades, bin_edges)\n",
    "                key = f\"{mA}:{gd}\"\n",
    "                hist_training[key] = counts_t / (sum(counts_t) + eps)\n",
    "                hist_true_inference[key] = counts_i / (sum(counts_i) + eps)\n",
    "                if sum(hist_training[key]) > 0 and sum(hist_true_inference[key]) > 0:\n",
    "                    training_groundtruth_hist_similarity[key] = 1 - jensenshannon(counts_t, counts_i)\n",
    "        with open(histogram_matrix_pfile, 'wb') as hdl:\n",
    "            variables = [hist_training, hist_true_inference, training_groundtruth_hist_similarity]\n",
    "            pickle.dump(variables, hdl, protocol=4)\n",
    "\n",
    "    # make graphs\n",
    "    f_width, f_height = 15., 12. # ideally, width=15, height=20 (but we will compress the y-axis to save space)\n",
    "    fig = plt.figure(figsize=(f_width, f_height))\n",
    "    count = 0\n",
    "\n",
    "    sim = training_groundtruth_hist_similarity\n",
    "    x_max = np.max(bin_representative_values)\n",
    "    for mA in uniq_inference_periods:\n",
    "        mA_mB_mC = f\"%02d_%02d_%02d\" % (mA, mA+1, mA+2)\n",
    "        for gd in uniq_domain_ids:\n",
    "            key = f\"{mA}:{gd}\"\n",
    "            # Convention: blue bars = training data, black rectangles = ground truth\n",
    "            plt.subplot(len(uniq_inference_periods), len(uniq_domain_ids), count+1) #rows, columns\n",
    "            plt.bar(bin_representative_values, hist_training[key], edgecolor=None, width=w)\n",
    "            plt.bar(bin_representative_values, hist_true_inference[key], facecolor=None, fill=False, edgecolor='k', width=w)\n",
    "            # Add annotation to show degree of similarity between training data and groundtruth distributions\n",
    "            if show_jsd_similarity:\n",
    "                annot = \"%.1f%%\" % (100 * sim[key]) if np.isfinite(sim.get(key,np.nan)) else \"\"\n",
    "                y_max = np.max(np.r_[hist_training[key], hist_true_inference[key]])\n",
    "                plt.text(x_max, y_max, annot, fontsize=7, color='#7f7f7f', ha='right', va='top')\n",
    "                if mA == uniq_inference_periods[-1] and gd == uniq_domain_ids[0]:\n",
    "                    explanation = '\\n\\n\\n% indicates JSD similarity between ' \\\n",
    "                                  'training data and groundtruth distributions'\n",
    "                    axL = plt.gca().secondary_xaxis(location=0)\n",
    "                    axL.set_xticks([0], labels=[explanation], fontsize=9, color='#7f7f7f', ha='left')\n",
    "            count += 1\n",
    "            if gd == uniq_domain_ids[0]:\n",
    "                plt.ylabel(str(mA))\n",
    "            plt.gca().set_yticks([])\n",
    "            if mA == uniq_inference_periods[-1]:\n",
    "                plt.xlabel(str(gd))\n",
    "            else:\n",
    "                plt.gca().set_xticks([])\n",
    "\n",
    "    fig.supylabel('Inference Period', fontsize=14)\n",
    "    fig.supxlabel('Geological Domain', y=-0.01, fontsize=14)\n",
    "    plt.suptitle(\"$\\\\bf{Copper\\ Grade\\ Probability\\ Mass\\ Functions}$\\n\"\n",
    "                 \"Histograms: Blue represents training data. Black represents groundtruth for predicted blocks\",\n",
    "                 y=1.0, fontsize=14)\n",
    "    # - make grids super-tight\n",
    "    plt.subplots_adjust(left=0.05, bottom=0.05, right=0.95, top=0.95, wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for fmt in ['pdf', 'png']:\n",
    "        plt.savefig(os.path.join(os.getcwd(), f\"histogram_matrix.{fmt}\"), bbox_inches='tight', pad_inches=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jensen-Shannon (JS) Histogram Difference Matrix\n",
    "- Graph 1: Grouped by Inference Period, then Geological Domain\n",
    "- Graph 2: Grouped by Geological Domain, then Inference Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#===============================\n",
    "rspace = 'learning_rotated'\n",
    "attribute = 'h(JS)'\n",
    "cmapBl = plt.get_cmap('Blues_r').copy()\n",
    "cmapBl.set_bad(color='#7f7f7f')\n",
    "cfg = {'title': r\"Jensen-Shannon histogram distances $h_{JS}$ (model vs groundtruth)\", 'fontsize': 14}\n",
    "#===============================\n",
    "\n",
    "# Order experiments by Inference Period (outer loop: mA), then by Domain (inner loop: gd)\n",
    "imshow_matrix_group_by_inference_domain(rspace, attribute, cmapBl, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmapBl = create_inverted_colormap(gamma=0.75, monochrome=True, lscm_lower=mpl.cm.Blues, flip_lower=True)\n",
    "cmapBl.set_bad(color='#dddddd')\n",
    "\n",
    "# Style 1: Use distinctive marker to represent NaNs to differentiate from high intensity values\n",
    "cfg = {'title': r\"Jensen-Shannon histogram distances $h_{JS}$ (model vs groundtruth)\",\n",
    "       'fontsize': 14, 'colorbar_title': attribute, 'decorate_nans': True,\n",
    "       'include_reference_models': True}\n",
    "imshow_matrix_group_by_domain_inference(rspace, attribute, cmapBl, cfg)\n",
    "\n",
    "# Style 2: Remove combinations where groundtruth is not available and modelling is not done\n",
    "#          Shift x-labels and line separator accordingly\n",
    "cfg = {'title': r\"Jensen-Shannon histogram distances $h_{JS}$ (model vs groundtruth)\",\n",
    "       'fontsize': 14, 'colorbar_title': attribute, 'hide_unmodelled_columns': True,\n",
    "       'include_reference_models': True}\n",
    "imshow_matrix_group_by_domain_inference(rspace, attribute, cmapBl, cfg)\n",
    "plt.savefig(os.path.join(os.getcwd(), f\"image-jensen-shannon-hist-dist.pdf\"), bbox_inches='tight', pad_inches=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#7f7f7f\">Experimental:</font> Bellman-Ford min-cost path via Dijkstra algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.graph\n",
    "\n",
    "def find_min_cost_path(rspace, attribute, cmap, cfg):\n",
    "    # Cost matrix\n",
    "    M = fetch_property(rspace, attribute, cfg)\n",
    "    # - apply custom cost function (if specified)\n",
    "    if cfg.get('dijkstra:cost_fn', None) is not None:\n",
    "        M = cfg['dijkstra:cost_fn'](M)\n",
    "    # - rectify cells with undefined values\n",
    "    nan_cost = cfg.get('nan_cost', np.nanmean(M))\n",
    "    M[~np.isfinite(M)] = nan_cost\n",
    "    objective = 'min'\n",
    "    # - option to maximise instead of minimise\n",
    "    if not cfg.get('dijkstra:minimise', True):\n",
    "        M = np.max(M) - M + 1\n",
    "        objective = 'max'\n",
    "\n",
    "    # Find optimal passage\n",
    "    nY = 2 + n_exponents #group_size\n",
    "    paths, costs = {}, {}\n",
    "    for g in range(int(M.shape[0] / nY)):\n",
    "        for i in range(g*nY, (g+1)*nY):\n",
    "            node_s = (i, 0)\n",
    "            for j in range(g*nY, (g+1)*nY):\n",
    "                node_f = (j, M.shape[1]-1)\n",
    "                k = f\"{i},{j}\"\n",
    "                paths[k], costs[k] = skimage.graph.route_through_array(M,\n",
    "                                     start=node_s, end=node_f, fully_connected=True)\n",
    "    # Rank paths by cost in ascending order\n",
    "    keys = list(paths.keys())\n",
    "    ranks = np.argsort([costs[k] for k in keys])\n",
    "    if not cfg.get('dijkstra:categorical_judgment', False):\n",
    "        print(f\"{attribute}: {objective}_key={keys[ranks[0]]}, {objective}_cost={costs[keys[ranks[0]]]}\")\n",
    "\n",
    "    # Prune preferences - retain only the best option for each unique source node\n",
    "    ordered_keys = [keys[r] for r in ranks]\n",
    "    source_nodes_encountered = set()\n",
    "    uniq_source_key_preferences = []\n",
    "    for k in ordered_keys:\n",
    "        src, dest = [int(i) for i in k.split(',')]\n",
    "        if src not in source_nodes_encountered:\n",
    "            if cfg.get('dijkstra:categorical_judgment', False):\n",
    "                uniq_source_key_preferences.append(k)\n",
    "                # discount remaining options in the current group\n",
    "                # instead, determine best path for other categories\n",
    "                g = int(np.floor(src / nY))\n",
    "                for i in range(g*nY, (g+1)*nY):\n",
    "                    source_nodes_encountered.add(i)\n",
    "            else:\n",
    "                uniq_source_key_preferences.append(k)\n",
    "                source_nodes_encountered.add(src)\n",
    "\n",
    "    # Specify number of paths to display\n",
    "    nD = min(max(cfg.get('num_paths', 3), 1), 6)\n",
    "    imshow_matrix_group_by_domain_inference(rspace, attribute, cmap, cfg)\n",
    "    for i, k, rgb in zip(range(nD), uniq_source_key_preferences[:nD],\n",
    "                         ['#fcf403','#fcba03','#fca903','#fc9003','#fc8003','#fc6703'][:nD]):\n",
    "        yx = np.array(paths[k])\n",
    "        plt.plot(yx[:,1], yx[:,0], '.-', color=rgb, linewidth=3-i*0.5)\n",
    "        if cfg.get('dijkstra:categorical_judgment', False):\n",
    "            print(f\"{attribute}: {objective}_key={k}, {objective}_cost={costs[k]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cfg_dijk = {'title': r\"Jensen-Shannon histogram distances $h_{JS}$ min-cost path\",\n",
    "            'hide_unmodelled_columns': True, 'include_reference_models': False, 'num_paths': 1}\n",
    "find_min_cost_path(rspace, 'h(JS)', cmapBl, cfg_dijk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wasserstein (EM) Histogram Difference Matrix\n",
    "\n",
    "Henceforth, grouped by geological domain, then inference period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#===============================\n",
    "rspace = 'learning_rotated'\n",
    "attribute = 'h(EM)'\n",
    "cmapBl = create_inverted_colormap(gamma=2.0, monochrome=True, lscm_lower=mpl.cm.Blues, flip_lower=True)\n",
    "cmapBl.set_bad(color='#7f7f7f')\n",
    "# Reusing configuration to preserve results from previous run\n",
    "cfg['title'] = r\"Wasserstein histogram distances $h_{EM}$ (model vs groundtruth)\"\n",
    "cfg['colorbar_title'] = attribute\n",
    "#===============================\n",
    "imshow_matrix_group_by_domain_inference(rspace, attribute, cmapBl, cfg)\n",
    "plt.savefig(os.path.join(os.getcwd(), f\"image-wasserstein-hist-dist.pdf\"), bbox_inches='tight', pad_inches=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_error(x, fmt='%.6f'):\n",
    "    xf = x.flatten()\n",
    "    mask = np.isfinite(xf)\n",
    "    return fmt % (np.std(xf[mask]) / np.sqrt(sum(mask)))\n",
    "\n",
    "def mean_of(x, fmt='%.6f'):\n",
    "    return fmt % np.nanmean(x)\n",
    "\n",
    "def median_of(x, fmt='%.6f'):\n",
    "    return fmt % np.nanmedian(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grade Prediction RMSE (model vs groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================\n",
    "attribute = 'RMSE'\n",
    "cmapBl = create_inverted_colormap(gamma=0.75, monochrome=True, lscm_lower=mpl.cm.Blues, flip_lower=True)\n",
    "cmapBl.set_bad(color='#7f7f7f')\n",
    "# Reusing configuration to preserve results from previous run\n",
    "cfg['title'] = r\"Grade Prediction Root Mean Squared Error\"\n",
    "cfg['colorbar_title'] = 'Uncorrelated RMS Error'\n",
    "#===============================\n",
    "\n",
    "imshow_matrix_group_by_domain_inference(rspace, attribute, cmapBl, cfg) #plt.grid('on')\n",
    "\n",
    "h_EM = cfg[f\"results:{rspace}_h(EM)\"]['M']\n",
    "h_JS = cfg[f\"results:{rspace}_h(JS)\"]['M']\n",
    "d_RMSE = cfg[f\"results:{rspace}_RMSE\"]['M']\n",
    "\n",
    "cached_h_EM = dict()\n",
    "cached_h_JS = dict()\n",
    "cached_d_RMSE = dict()\n",
    "\n",
    "print(\"\\nAverage histogram difference h(Jensen-Shannon)\")\n",
    "model_families = ['SimpleKriging','OrdinaryKriging','GaussianProcess(G)-CRF','GaussianProcess(L)-SGS']\n",
    "for i,cat in zip(range(4), model_families):\n",
    "    values = h_JS[i*(n_exponents+2):(i+1)*(n_exponents+2),:]\n",
    "    cached_h_EM[cat] = values.flatten()\n",
    "    print(f\"- {'{:<30}'.format(cat + ' models')} {mean_of(values)} +/- {standard_error(values)}\")\n",
    "\n",
    "print(\"\\nAverage histogram difference h(Wasserstein.EM)\")\n",
    "for i,cat in zip(range(4), model_families):\n",
    "    values = h_EM[i*(n_exponents+2):(i+1)*(n_exponents+2),:]\n",
    "    cached_h_JS[cat] = values.flatten()\n",
    "    print(f\"- {'{:<30}'.format(cat + ' models')} {mean_of(values)} +/- {standard_error(values)}\")\n",
    "\n",
    "print(\"\\nAverage prediction RMSE\")\n",
    "for i,cat in zip(range(4), model_families):\n",
    "    values = d_RMSE[i*(n_exponents+2):(i+1)*(n_exponents+2),:]\n",
    "    cached_d_RMSE[cat] = values.flatten()\n",
    "    print(f\"- {'{:<30}'.format(cat + ' models')} {mean_of(values)} +/- {standard_error(values)}\")\n",
    "\n",
    "vect_h_EM = h_EM.flatten()\n",
    "vect_h_JS = h_JS.flatten()\n",
    "vect_d_RMSE = d_RMSE.flatten()\n",
    "valid = np.isfinite(vect_h_EM + vect_h_JS + vect_d_RMSE)\n",
    "print(\"\\nPearson correlation\")\n",
    "print(f\"- rho(h(Wasserstein.EM),RMSE) = {np.corrcoef(vect_h_EM[valid], vect_d_RMSE[valid])[0,1]}\\n\")\n",
    "\n",
    "# significance testing using the dependent T-test\n",
    "# https://www.investopedia.com/terms/t/t-test.asp\n",
    "ft = lambda x: np.round(x,6)\n",
    "print(\"Dependent T-test on h(Jensen-Shannon) w.r.t. GaussianProcess(L)-SGS\")\n",
    "for i,cat in zip(range(4), [m for m in model_families if m != 'GaussianProcess(L)-SGS']):\n",
    "    r = t_test(cached_h_JS[cat], cached_h_JS['GaussianProcess(L)-SGS'], ha='greater')\n",
    "    print(f\"- {cat}: T={r[0]}, p={r[1]}, df={r[2]}, CI=[{r[3]},{r[4]}]\")\n",
    "\n",
    "print(\"Dependent T-test on h(Wasserstein.EM) w.r.t. GaussianProcess(L)-SGS\")\n",
    "for i,cat in zip(range(4), [m for m in model_families if m != 'GaussianProcess(L)-SGS']):\n",
    "    r = t_test(cached_h_EM[cat], cached_h_EM['GaussianProcess(L)-SGS'], ha='greater')\n",
    "    print(f\"- {cat}: T={r[0]}, p={r[1]}, df={r[2]}, CI=[{r[3]},{r[4]}]\")\n",
    "\n",
    "print(\"Dependent T-test on prediction RMSE w.r.t. GaussianProcess(L)-SGS\")\n",
    "for i,cat in zip(range(4), [m for m in model_families if m != 'GaussianProcess(L)-SGS']):\n",
    "    r = t_test(cached_d_RMSE[cat], cached_d_RMSE['GaussianProcess(L)-SGS'], ha='greater')\n",
    "    print(f\"- {cat}: T={r[0]}, p={r[1]}, df={r[2]}, CI=[{r[3]},{r[4]}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Fidelity Matrix based on Variogram-Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rspace = 'learning_rotated'\n",
    "attribute = 'Variogram Ratios'\n",
    "\n",
    "# colour version\n",
    "cmapBi = create_inverted_colormap(gamma=0.75, monochrome=False, lscm_upper=mpl.cm.Purples, #YlOrBr\n",
    "                                  lscm_lower=mpl.cm.Reds, flip_lower=False, flip_upper=False)\n",
    "cmapBi.set_bad(color='#ffffff')\n",
    "cfg_sf = {'title': r\"Variogram Ratios (model vs groundtruth)\",\n",
    "          'v_min': 0, 'v_max': 2., 'fontsize': 14, 'colorbar_title': attribute,\n",
    "          'hide_unmodelled_columns': True, 'decorate_nans': True,\n",
    "          'include_reference_models': True}\n",
    "imshow_matrix_group_by_domain_inference(rspace, attribute, cmapBi, cfg_sf)\n",
    "plt.savefig(os.path.join(os.getcwd(), f\"image-variogram-ratios.pdf\"), bbox_inches='tight', pad_inches=0.05)\n",
    "\n",
    "# gray-scale version\n",
    "attribute = 'Spatial Fidelity'\n",
    "cmapK = create_inverted_colormap(gamma=1.5, monochrome=True, lscm_upper=mpl.cm.gray,\n",
    "                                  lscm_lower=mpl.cm.gray, flip_lower=True, flip_upper=True)\n",
    "cmapK.set_bad(color='#ffffff')\n",
    "cfg_sf = {'title': r\"Variogram-derived Spatial Fidelity (model vs groundtruth)\",\n",
    "          'v_min': -0.01, 'v_max': 1., 'fontsize': 14, 'colorbar_title': attribute,\n",
    "          'hide_unmodelled_columns': True, 'decorate_nans': True,\n",
    "          'include_reference_models': True}\n",
    "imshow_matrix_group_by_domain_inference(rspace, attribute, cmapK, cfg_sf)\n",
    "plt.savefig(os.path.join(os.getcwd(), f\"image-variogram-based-spatial-fidelity.pdf\"), bbox_inches='tight', pad_inches=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_dijk = {'title': r\"Variogram-derived $Spatial\\ Fidelity$ (model vs groundtruth)\",\n",
    "            'v_min': 0, 'v_max': 2., 'fontsize': 14, 'colorbar_title': attribute,\n",
    "            'hide_unmodelled_columns': True, 'decorate_nans': True,\n",
    "            'include_reference_models': False, 'num_paths': 1,\n",
    "            'dijkstra:minimise': False, 'dijkstra:cost_fn': lambda x: np.sqrt(1 - np.abs(x - 1))}\n",
    "find_min_cost_path(rspace, 'Spatial Fidelity', cmapK, cfg_dijk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want a convex symmetric cost function about x=1\n",
    "# The desired expression is y = sqrt(1 - |x-1|)\n",
    "attribute = 'Spatial Fidelity'\n",
    "cost_sf = lambda x: np.sqrt(1 - np.abs(np.minimum(x, 2) - 1))\n",
    "C_variogram_ratios = cfg_sf[f\"results:{rspace}_{attribute}\"]['M']\n",
    "C_spatial_fidelity = cost_sf(cfg_sf[f\"results:{rspace}_{attribute}\"]['M'])\n",
    "\n",
    "cached_variogram_ratios = dict()\n",
    "cached_spatial_fidelity = dict()\n",
    "print(\"\\nAverage Variogram Ratios\")\n",
    "for i,cat in enumerate(model_families):\n",
    "    cells = C_variogram_ratios[i*(n_exponents+2):(i+1)*(n_exponents+2),:]\n",
    "    cached_variogram_ratios[cat] = cells.flatten()\n",
    "    print(f\"- {'{:<30}'.format(cat + ' models')} {mean_of(cells)} +/- {standard_error(cells)}\")\n",
    "\n",
    "print(\"\\nAverage Spatial Fidelity\")\n",
    "for i,cat in enumerate(model_families):\n",
    "    cells = C_spatial_fidelity[i*(n_exponents+2):(i+1)*(n_exponents+2),:]\n",
    "    cached_spatial_fidelity[cat] = cells.flatten()\n",
    "    print(f\"- {'{:<30}'.format(cat + ' models')} {mean_of(cells)} +/- {standard_error(cells)}\")\n",
    "\n",
    "# significance testing using the dependent T-test\n",
    "ft = lambda x: np.round(x,6)\n",
    "print(\"Dependent T-test on Spatial Fidelity w.r.t. GaussianProcess(L)-SGS\")\n",
    "for i,cat in zip(range(4), [m for m in model_families if m != 'GaussianProcess(L)-SGS']):\n",
    "    r = t_test(cached_spatial_fidelity[cat], cached_spatial_fidelity['GaussianProcess(L)-SGS'], ha='less')\n",
    "    print(f\"- {cat}: T={r[0]}, p={r[1]}, df={r[2]}, CI=[{r[3]},{r[4]}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#cc0044\">Uncertainty-based Measures</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deutsch's Accuracy for Probabilistic Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rspace = 'learning_rotated'\n",
    "attribute = 'Accuracy(.05)'\n",
    "\n",
    "cmapRd = create_inverted_colormap(gamma=1, monochrome=True,\n",
    "                                  lscm_lower=mpl.cm.Reds, flip_lower=False)\n",
    "cmapRd.set_bad(color='#7f7f7f')\n",
    "cfg_a = {'title': r\"Deutsch's Accuracy for Probabilistic Predictions\",\n",
    "         'v_min': 0, 'v_max': 1., 'fontsize': 14, 'colorbar_title': attribute,\n",
    "         'hide_unmodelled_columns': True, 'decorate_nans': True,\n",
    "         'include_reference_models': False}\n",
    "imshow_matrix_group_by_domain_inference(rspace, attribute, cmapRd, cfg_a)\n",
    "\n",
    "um_A = cfg_a[f\"results:{rspace}_{attribute}\"]['M']\n",
    "\n",
    "cached_A = dict()\n",
    "print(\"\\nAverage Accuracy of Probabilistic Predictions\")\n",
    "# Special note: we exclude rows labelled '*_[SGS|CRF]_from_{m}' where m=2,4,8\n",
    "mask = [True]*2 + [False]*3 + [True]*(n_exponents-3) \n",
    "for i,cat in enumerate(model_families):\n",
    "    rows = np.arange(i*(n_exponents+2),(i+1)*(n_exponents+2))[mask]\n",
    "    values = um_A[rows,:]\n",
    "    cached_A[cat] = values.flatten()\n",
    "    print(f\"- {'{:<30}'.format(cat + ' models')} {mean_of(values)} +/- {standard_error(values)}\")\n",
    "plt.savefig(os.path.join(os.getcwd(), f\"image-distribution-accuracy.pdf\"), bbox_inches='tight', pad_inches=0.05)\n",
    "\n",
    "# significance testing using the dependent T-test\n",
    "ft = lambda x: np.round(x,6)\n",
    "print(\"Dependent T-test on Deutsch Accuracy w.r.t. GaussianProcess(L)-SGS\")\n",
    "for i,cat in zip(range(4), [m for m in model_families if m != 'GaussianProcess(L)-SGS']):\n",
    "    r = t_test(cached_A[cat], cached_A['GaussianProcess(L)-SGS'], ha='less')\n",
    "    print(f\"- {cat}: T={r[0]}, p={r[1]}, df={r[2]}, CI=[{r[3]},{r[4]}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Precision for Probabilistic Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rspace = 'learning_rotated'\n",
    "attribute = 'Precision'\n",
    "\n",
    "cmapRd = create_inverted_colormap(gamma=1, monochrome=True,\n",
    "                                  lscm_lower=mpl.cm.Reds, flip_lower=False)\n",
    "cmapRd.set_bad(color='#7f7f7f')\n",
    "cfg_p = {'title': r\"Conditional Precision for Probabilistic Predictions\",\n",
    "         'v_min': 0, 'v_max': 1., 'fontsize': 14, 'colorbar_title': attribute,\n",
    "         'hide_unmodelled_columns': True, 'decorate_nans': True,\n",
    "         'include_reference_models': False}\n",
    "imshow_matrix_group_by_domain_inference(rspace, attribute, cmapRd, cfg_p)\n",
    "\n",
    "um_P = cfg_p[f\"results:{rspace}_{attribute}\"]['M']\n",
    "\n",
    "cached_P = dict()\n",
    "print(\"\\nAverage Precision of Probabilistic Predictions\")\n",
    "mask = [True]*2 + [True]*3 + [True]*(n_exponents-3)\n",
    "for i,cat in enumerate(model_families):\n",
    "    rows = np.arange(i*(n_exponents+2),(i+1)*(n_exponents+2))[mask]\n",
    "    select = um_A[rows,:] > 0.0\n",
    "    values = um_P[rows,:]\n",
    "    cached_P[cat] = values.flatten()\n",
    "    print(f\"- {'{:<30}'.format(cat + ' models')} {mean_of(values[select])} +/- {standard_error(values[select])}\")\n",
    "plt.savefig(os.path.join(os.getcwd(), f\"image-distribution-precision.pdf\"), bbox_inches='tight', pad_inches=0.05)\n",
    "\n",
    "# significance testing using the dependent T-test\n",
    "ft = lambda x: np.round(x,6)\n",
    "print(\"Dependent T-test on Precision w.r.t. GaussianProcess(L)-SGS\")\n",
    "for i,cat in zip(range(4), [m for m in model_families if m != 'GaussianProcess(L)-SGS']):\n",
    "    r = t_test(cached_P[cat], cached_P['GaussianProcess(L)-SGS'], ha='less')\n",
    "    print(f\"- {cat}: T={r[0]}, p={r[1]}, df={r[2]}, CI=[{r[3]},{r[4]}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Consensus for Probabilistic Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rspace = 'learning_rotated'\n",
    "attribute = 'Consensus'\n",
    "\n",
    "cmapRd = create_inverted_colormap(gamma=0.5, monochrome=True,\n",
    "                                  lscm_lower=mpl.cm.Reds, flip_lower=False)\n",
    "cmapRd.set_bad(color='#7f7f7f')\n",
    "cfg_l = {'title': r\"Local Consensus for Probabilistic Predictions\",\n",
    "         'v_min': 0, 'v_max': 1., 'fontsize': 14, 'colorbar_title': attribute,\n",
    "         'hide_unmodelled_columns': True, 'decorate_nans': True,\n",
    "         'include_reference_models': False}\n",
    "imshow_matrix_group_by_domain_inference(rspace, attribute, cmapRd, cfg_l)\n",
    "\n",
    "um_L = cfg_l[f\"results:{rspace}_{attribute}\"]['M']\n",
    "um_sL = fetch_property(rspace, '|s|_L', cfg_l)\n",
    "um_sU = fetch_property(rspace, '|s|_U', cfg_l)\n",
    "\n",
    "mask = [True]*2 + [True]*3 + [True]*(n_exponents-3)\n",
    "cached_L = dict()\n",
    "print(\"\\nLocal Consensus for Probabilistic Predictions\")\n",
    "for i,cat in enumerate(model_families):\n",
    "    rows = np.arange(i*(n_exponents+2),(i+1)*(n_exponents+2))[mask]\n",
    "    values = um_L[rows,:]\n",
    "    cached_L[cat] = values.flatten()\n",
    "    print(f\"- {'{:<30}'.format(cat + ' models')} median={median_of(values)} \"\n",
    "          f\"[qL={mean_of(um_sL[rows,:])}, qU={mean_of(um_sU[rows,:])}]\")\n",
    "plt.savefig(os.path.join(os.getcwd(), f\"image-distribution-consensus.pdf\"), bbox_inches='tight', pad_inches=0.05)\n",
    "\n",
    "# significance testing using the dependent T-test\n",
    "ft = lambda x: np.round(x,6)\n",
    "print(\"Dependent T-test on Consensus w.r.t. GaussianProcess(L)-SGS\")\n",
    "for i,cat in zip(range(4), [m for m in model_families if m != 'GaussianProcess(L)-SGS']):\n",
    "    r = t_test(cached_L[cat], cached_L['GaussianProcess(L)-SGS'], ha='less')\n",
    "    print(f\"- {cat}: T={r[0]}, p={r[1]}, df={r[2]}, CI=[{r[3]},{r[4]}]\")\n",
    "\n",
    "# significance testing using the Welch's T-test\n",
    "print(\"Independent Walsh's T-test on Consensus w.r.t. GaussianProcess(L)-SGS\")\n",
    "for i,cat in zip(range(4), [m for m in model_families if m != 'GaussianProcess(L)-SGS']):\n",
    "    r = t_test(cached_L[cat], cached_L['GaussianProcess(L)-SGS'], ha='less', which='welch')\n",
    "    print(f\"- {cat}: T={r[0]}, p={r[1]}, df={r[2]}, CI=[{r[3]},{r[4]}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goodness of Probabilistic Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rspace = 'learning_rotated'\n",
    "attribute = 'Goodness'\n",
    "\n",
    "cmapRd = create_inverted_colormap(gamma=1, monochrome=True,\n",
    "                                  lscm_lower=mpl.cm.Reds, flip_lower=False)\n",
    "cmapRd.set_bad(color='#7f7f7f')\n",
    "cfg_g = {'title': r\"Goodness of Probabilistic Predictions\",\n",
    "         'v_min': 0, 'v_max': 1., 'fontsize': 14, 'colorbar_title': attribute,\n",
    "         'hide_unmodelled_columns': True, 'decorate_nans': True,\n",
    "         'include_reference_models': False}\n",
    "imshow_matrix_group_by_domain_inference(rspace, attribute, cmapRd, cfg_g)\n",
    "\n",
    "um_G = cfg_g[f\"results:{rspace}_{attribute}\"]['M']\n",
    "\n",
    "mask = [True]*2 + [True]*3 + [True]*(n_exponents-3)\n",
    "cached_G = dict()\n",
    "print(\"\\nAverage Goodness of Probabilistic Predictions\")\n",
    "for i,cat in enumerate(model_families):\n",
    "    rows = np.arange(i*(n_exponents+2),(i+1)*(n_exponents+2))[mask]\n",
    "    values = um_G[rows,:]\n",
    "    cached_G[cat] = values.flatten()\n",
    "    print(f\"- {'{:<30}'.format(cat + ' models')} {mean_of(values)} +/- {standard_error(values)}\")\n",
    "plt.savefig(os.path.join(os.getcwd(), f\"image-distribution-goodness.pdf\"), bbox_inches='tight', pad_inches=0.05)\n",
    "\n",
    "# significance testing using the dependent T-test\n",
    "ft = lambda x: np.round(x,6)\n",
    "print(\"Dependent T-test on Goodness w.r.t. GaussianProcess(L)-SGS\")\n",
    "for i,cat in zip(range(4), [m for m in model_families if m != 'GaussianProcess(L)-SGS']):\n",
    "    r = t_test(cached_G[cat], cached_G['GaussianProcess(L)-SGS'], ha='less')\n",
    "    print(f\"- {cat}: T={r[0]}, p={r[1]}, df={r[2]}, CI=[{r[3]},{r[4]}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_dijk = {'title': r\"Goodness of Probabilistic Predictions\",\n",
    "            'v_min': 0, 'v_max': 1., 'fontsize': 14, 'colorbar_title': attribute,\n",
    "            'hide_unmodelled_columns': True, 'decorate_nans': True,\n",
    "            'include_reference_models': False, 'num_paths': 1, 'dijkstra:minimise': False}\n",
    "cmapGray = create_inverted_colormap(gamma=1, monochrome=True,\n",
    "                                    lscm_lower=mpl.cm.gray, flip_lower=True)\n",
    "#find_min_cost_path(rspace, 'Goodness', cmapGray, cfg_dijk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tightness of Probabilistic Predictions (Normalised by $\\sigma_Y$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rspace = 'learning_rotated'\n",
    "attribute = 'Tightness'\n",
    "\n",
    "cmapRd = create_inverted_colormap(gamma=0.4, monochrome=True,\n",
    "                                  lscm_lower=mpl.cm.Blues, flip_lower=True)\n",
    "cmapRd.set_bad(color='#ffffff')\n",
    "cfg_t = {'title': r\"Tightness of Probabilistic Predictions (Normalised by $\\sigma_Y$)\",\n",
    "         'v_min': 0, 'v_max': 1., 'fontsize': 14, 'colorbar_title': attribute,\n",
    "         'hide_unmodelled_columns': True, 'decorate_nans': True,\n",
    "         'include_reference_models': False}\n",
    "imshow_matrix_group_by_domain_inference(rspace, attribute, cmapRd, cfg_t)\n",
    "\n",
    "um_T = cfg_t[f\"results:{rspace}_{attribute}\"]['M']\n",
    "\n",
    "mask = [True]*2 + [True]*2 + [True]*(n_exponents-2) \n",
    "cached_T = dict()\n",
    "print(\"\\nAverage Tightness of Probabilistic Predictions\")\n",
    "for i,cat in enumerate(model_families):\n",
    "    rows = np.arange(i*(n_exponents+2),(i+1)*(n_exponents+2))[mask]\n",
    "    values = um_T[rows,:]\n",
    "    cached_T[cat] = values.flatten()\n",
    "    print(f\"- {'{:<30}'.format(cat + ' models')} {mean_of(values)} +/- {standard_error(values)}\")\n",
    "plt.savefig(os.path.join(os.getcwd(), f\"image-distribution-tightness.pdf\"), bbox_inches='tight', pad_inches=0.05)\n",
    "\n",
    "# significance testing using the dependent T-test\n",
    "print(\"Dependent T-test on Tightness w.r.t. GaussianProcess(L)-SGS\")\n",
    "for i,cat in zip(range(4), [m for m in model_families if m != 'GaussianProcess(L)-SGS']):\n",
    "    r = t_test(cached_T[cat], cached_T['GaussianProcess(L)-SGS'], ha='greater')\n",
    "    print(f\"- {cat}: T={r[0]}, p={r[1]}, df={r[2]}, CI=[{r[3]},{r[4]}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_dijk = {'title': r\"Tightness of Probabilistic Predictions (Normalised by $\\sigma_Y$)\",\n",
    "            'v_min': 0, 'v_max': 1, 'fontsize': 14, 'colorbar_title': attribute,\n",
    "            'hide_unmodelled_columns': True, 'decorate_nans': True,\n",
    "            'include_reference_models': False, 'num_paths': 1, 'dijkstra:categorical_judgment': True}\n",
    "cmapGray = create_inverted_colormap(gamma=1, monochrome=True,\n",
    "                                    lscm_lower=mpl.cm.gray, flip_lower=True)\n",
    "#find_min_cost_path(rspace, 'Tightness', cmapGray, cfg_dijk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average statistics weighted by domain sample count\n",
    "- Due to the complexity of the Cochran formula for weighted standard error of mean (wSEM), we will not be using \"add-and-save\" accumulation to save memory. Instead, we will maintain the (w*M)[j] and w[j] data points for j=1:n_obsv, where n_obsv = len(uniq_domains)*len(uniq_inference_periods)\n",
    "- We perform an extra step and computes Spatial Fidelity as sqrt(1 - |x - 1|)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weighted_stats(rspace, cfg={}):\n",
    "    \"\"\"\n",
    "    @brief  Compute sample_weights and weighted average stats M where\n",
    "            M.shape=(rows,cols) where rows = #model, cols = (#domain * #inference_period)\n",
    "    @note   Spatial Fidelity here refers to the cost \n",
    "    \"\"\"\n",
    "    n_ip = len(uniq_inference_periods)\n",
    "    n_gd = len(uniq_domain_ids)\n",
    "    n_obsv = len(uniq_inference_periods) * len(uniq_domain_ids)\n",
    "    n_rows = len(df_example)\n",
    "    n_cols = df_example.shape[1]\n",
    "\n",
    "    array_wX = np.zeros((n_rows, n_cols, n_obsv))\n",
    "    array_w = np.zeros((n_rows, n_cols, n_obsv))\n",
    "    group_by = cfg.get('group_by', 'domain,inference')\n",
    "    eps = cfg.get('eps', 1e-6)\n",
    "\n",
    "    if group_by == 'domain,inference':\n",
    "        outer_vars = uniq_domain_ids\n",
    "        inner_vars = uniq_inference_periods\n",
    "    else:\n",
    "        outer_vars = uniq_inference_periods\n",
    "        inner_vars = uniq_domain_ids\n",
    "\n",
    "    obsv = 0\n",
    "    for vo in outer_vars:\n",
    "        for vi in inner_vars:\n",
    "            gd = vo if group_by == 'domain,inference' else vi\n",
    "            mA = vi if group_by == 'domain,inference' else vo\n",
    "            mA_mB_mC = f\"%02d_%02d_%02d\" % (mA, mA+1, mA+2)\n",
    "            analysis_csv = os.path.join(result_dir, rspace, f\"{mA_mB_mC}\", f\"analysis-{gd}.csv\")\n",
    "            record = df_gdp.query(f\"mA == {mA} & domain == {gd}\")\n",
    "            if record.shape[0] > 0:\n",
    "                n = record.n_inference_pts.values[0]\n",
    "                df = pd.read_csv(analysis_csv, index_col=0, header=0)\n",
    "                df.rename(columns={'Likelihood': 'Consensus'}, inplace=True)",
    "                cost_f = lambda x: np.sqrt(1 - np.abs(x - 1)) if np.isfinite(x) else np.nan\n",
    "                df['Spatial Fidelity'] = df['Spatial Fidelity'].apply(cost_f)\n",
    "                X = df.values\n",
    "                array_w[:,:,obsv] = n * np.isfinite(X)\n",
    "                array_wX[:,:,obsv] = array_w[:,:,obsv] * X\n",
    "            obsv += 1\n",
    "    array_wX[~np.isfinite(array_wX)] = 0\n",
    "\n",
    "    # Compute (wSEM)^2, see (Gatz and Smith, 1995)\n",
    "    n = np.sum(array_w, axis=2)\n",
    "    w = array_w\n",
    "    wX = array_wX\n",
    "    X_bar_w = np.sum(wX, axis=2) / np.sum(w, axis=2)\n",
    "    w_bar = np.mean(w, axis=2)\n",
    "    t1 = (n / ((n-1) * np.sum(w, axis=2)**2))\n",
    "    t2 = np.sum((wX - (w_bar * X_bar_w)[:,:,np.newaxis])**2, axis=2)\n",
    "    t3 = - 2 * X_bar_w * np.sum((w - w_bar[:,:,np.newaxis]) * (wX - (w_bar * X_bar_w)[:,:,np.newaxis]), axis=2)\n",
    "    t4 = X_bar_w**2 * np.sum((w - w_bar[:,:,np.newaxis])**2, axis=2)\n",
    "    wSEM = np.sqrt(t1 * (t2 + t3 + t4))\n",
    "\n",
    "    # Present results in DataFrame\n",
    "    df_combined = df.copy()\n",
    "    df_weighted_mean = df.copy()\n",
    "    df_weighted_se = df.copy()\n",
    "    for j, c in enumerate(df.columns):\n",
    "        df_combined.loc[:, c] = ['%.4f (%.4f)' % (x,se) for x, se in zip(X_bar_w[:,j], wSEM[:,j])]\n",
    "        df_weighted_mean.loc[:, c] = ['%.4f' % (x) for x in X_bar_w[:,j]]\n",
    "        df_weighted_se.loc[:, c] = ['%.4f' % (se) for se in wSEM[:,j]]\n",
    "\n",
    "    return (df_weighted_mean.reindex(ordered_model_names),\n",
    "            df_weighted_se.reindex(ordered_model_names),\n",
    "            df_combined.reindex(ordered_model_names),\n",
    "            X_bar_w, wSEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_weighted_mean, df_weighted_se, df_combined, wX, wSE = compute_weighted_stats(rspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weighted_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSample-weighted Average Statistics\")\n",
    "\n",
    "column_A = -4\n",
    "rows_ignored = np.array([i*(n_exponents+2)+np.r_[2,3] for i in np.arange(4)]).flatten()\n",
    "\n",
    "values = np.array([[eval(col) for col in row] for row in df_weighted_mean.values])\n",
    "values[rows_ignored, column_A] = np.nan\n",
    "family_mean = []\n",
    "family_se = []\n",
    "\n",
    "for i,cat in enumerate(model_families):\n",
    "    rows = np.arange(i*(n_exponents+2),(i+1)*(n_exponents+2))[mask]\n",
    "    block = values[rows,:]\n",
    "    family_mean.append(np.nanmean(block, axis=0))\n",
    "    family_se.append(np.nanstd(block, axis=0) / np.sqrt(np.sum(np.isfinite(block), axis=0)))\n",
    "\n",
    "df_family_means = pd.DataFrame(data=family_mean, index=model_families, columns=df_weighted_mean.columns)\n",
    "df_family_stderrs = pd.DataFrame(data=family_se, index=model_families, columns=df_weighted_mean.columns)\n",
    "\n",
    "print('Family means')\n",
    "df_family_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Family standard errors')\n",
    "df_family_stderrs"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
