{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Uncertainty and Predictive Performance of Probabilistic Models\n",
    "\n",
    "<b>Raymond Leung, Alexander Lowe, Arman Melkumyan</b><br>\n",
    "Rio Tinto Centre, Faculty of Engineering<br>\n",
    "The University of Sydney, 2024\n",
    "\n",
    "`SPDX-FileCopyrightText: 2024 Raymond Leung and Alexander Lowe <raymond.leung@sydney.edu.au>`<br>\n",
    "`SPDX-License-Identifier: `[`BSD-3-Clause`](https://opensource.org/license/BSD-3-Clause)<br>\n",
    "\n",
    "\n",
    "### Demonstration code\n",
    "This notebook clarifies how the methods described in [README.md](../README.md) are used to build and evaluate probabilistic models. It provides a template for running the experiments for a given domain and inference period. It\n",
    "- Generates all models of interest: SK, OK, SK-SGS, OK-SGS, GP(L), GP(G) GP-SGS, GP-CRF.\n",
    "- Computes the histogram, variogram and uncertainty-based statistics (described below) and performs rank analysis.\n",
    "- Produces graphs and figures, including the kappa-accuracy and interval tightness plots, $\\hat{\\mu}$, $\\hat{\\sigma}$, $s(\\hat{\\mu},\\hat{\\sigma}\\mid\\mu_0)$ and variograms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#cc0066\">Part 1: Models Construction</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Python modules\n",
    "import ast\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skgstat as skg\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "sys.path.append(os.getcwd().replace('notebook', 'code'))\n",
    "\n",
    "from IPython.display import Image\n",
    "from collections import OrderedDict\n",
    "from matplotlib import pyplot as plt\n",
    "from pdb import set_trace as bp\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from scipy.stats import norm\n",
    "\n",
    "from gstatsim3d_utils import make_scatter_2d, timeit\n",
    "from gstatsim3d_gaussian_process import GPManager\n",
    "from gstatsim3d_kriging import KrigingRegression, KrigingManager\n",
    "\n",
    "from rtc_trend_alignment import compute_any_rotation_and_scaling_matrix\n",
    "from rtc_evaluation_metrics import *\n",
    "from rtc_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mandatory parameters specified by user\n",
    "#===============================================================================\n",
    "#  The `inference_prefix` is used to define the modelling period \"mA_mB_mC\"\n",
    "#  which indicates an intent to estimate the target variable (copper grade)\n",
    "#  for a 3 month period starting from month \"mA\" using previously gathered data\n",
    "inference_prefix = mA = 4\n",
    "\n",
    "#  The `domain_id` limits the scope of the regionalised variable to certain\n",
    "#  spatial domain based on geological conditions established by mine geologists.\n",
    "domain_id = 2310\n",
    "\n",
    "#  Optional config overrides\n",
    "overrides = {\n",
    "    'inference_type': 'future-bench-prediction',\n",
    "    'simulation:num': 32, #number of simulations used in our experiments was 128\n",
    "    'kriging:transform_data': True, #rotate and scale according to domain orientation\n",
    "    'gp:learning_inference_in_rotated_space': True\n",
    "}\n",
    "#===============================================================================\n",
    "\n",
    "specs = {'mA': mA, 'domain_id': domain_id}\n",
    "specs.update(overrides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mA_mB_mC = '%02d_%02d_%02d' % (mA, mA+1, mA+2)\n",
    "model_names = []\n",
    "model_exec_times = []\n",
    "learn_times = []\n",
    "inference_times = []\n",
    "\n",
    "# Global configuration\n",
    "cfg_rs = dict()\n",
    "default_code_dir = os.getcwd().replace('notebook', 'code')\n",
    "default_data_dir = default_code_dir.replace('code', 'data')\n",
    "default_result_dir = default_code_dir.replace('code', 'results')\n",
    "cfg_rs['info:data_dir'] = specs.get('info:data_dir', default_data_dir)\n",
    "cfg_rs['info:result_dir'] = specs.get('info:result_dir', default_result_dir)\n",
    "cfg_rs['gp:associate_plunge_rotation_with_x'] = False\n",
    "specs.update(cfg_rs)\n",
    "data_path = f\"{cfg_rs['info:data_dir']}/{mA_mB_mC}\"\n",
    "\n",
    "# Check if variogram fitting and GP learning occur in rotated space\n",
    "subdir = check_learning_rotation_status(specs)\n",
    "result_path = f\"{cfg_rs['info:result_dir']}/{subdir}/{mA_mB_mC}\"\n",
    "figure_path = f\"{cfg_rs['info:result_dir']}/{subdir}/{mA_mB_mC}/figs\"\n",
    "os.makedirs(figure_path, exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "df_bh = pd.read_csv(f\"{data_path}/blastholes_tagged.csv\")\n",
    "df_bh = df_bh.rename(columns = {'EAST': 'X', 'NORTH': 'Y', 'RL': 'Z', 'PL_CU': 'V'})\n",
    "df_domain_x = pd.DataFrame(columns=['X','Y','Z','V']) #not using exploration assays\n",
    "df_domain_bh = df_bh[(df_bh['lookup_domain'] == domain_id) & np.isfinite(df_bh['V'].values)]\n",
    "min_v, max_v = np.min(df_domain_bh['V'].values), np.max(df_domain_bh['V'].values)\n",
    "R, S = compute_any_rotation_and_scaling_matrix(cfg_rs, domain_id)\n",
    "\n",
    "# Inference locations\n",
    "df_k = pd.read_csv(f\"{data_path}/blocks_to_estimate_tagged.csv\")\n",
    "df_domain_infer = pd.DataFrame(df_k[df_k['domain'] == domain_id])\n",
    "ground_truth = df_domain_infer['cu_bh_nn'].values\n",
    "\n",
    "# Visualise blastholes grades\n",
    "# - Add `savefile=os.path.join(figure_path, f\"blastholes_grade_{domain_id}.pdf\")`\n",
    "#   to save figure as a PDF file. This, however, will suppress the display.\n",
    "_ = make_scatter_2d(df_domain_bh['X'], df_domain_bh['Y'], df_domain_bh['V'],\n",
    "                min_v, max_v, symbsiz=50, palette='YlOrRd', cbtitle=\"Cu grade\", symbol='s',\n",
    "                graphtitle=f\"Blastholes Cu grade - known samples for {mA_mB_mC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Kriging and simulation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Kriging and simulation parameters\n",
    "cfg_krige = create_kriging_config(R, S, specs)\n",
    "print(f\"Processing {mA_mB_mC}, domain {domain_id}\")\n",
    "print(f\"- Training uses {len(df_domain_bh)} blastholes\")\n",
    "print(f\"- Inferencing: {len(df_domain_infer)} blocks\")\n",
    "if subdir == 'learning_rotated':\n",
    "    print(f\"Rotation matrix R={R}\\nScaling vector S={S}\")\n",
    "if cfg_krige['simulation:num'] > 256:\n",
    "    specs['simulation:num'] = cfg_krige['simulation:num'] = 256\n",
    "    print('The maximum number of simulations is capped at 256')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique 1: OK Sequential Gaussian Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_oksgs = copy.deepcopy(cfg_krige)\n",
    "oksgs_abbrev = ''.join([x[0] for x in cfg_oksgs['kriging:type'].split('_')]) + 'sgs'\n",
    "oksgs_abbrev += 'r' if cfg_oksgs['kriging:transform_data'] else ''\n",
    "oksgs_csv = f\"{result_path}/predictions-{oksgs_abbrev}-{domain_id}.csv\"\n",
    "\n",
    "model_names.append('OK-SGS')\n",
    "if os.path.isfile(oksgs_csv):\n",
    "    print(f\"Reading {oksgs_csv}\")\n",
    "    cfg_oksgs['bypass_simulation'] = True   # only retrieve meta-data such as column labels\n",
    "    KrigingManager.kriging_sequential_simulations(df_domain_bh, df_domain_infer, cfg_oksgs)\n",
    "    df_oksgs = pd.read_csv(oksgs_csv, index_col=0, header=0)\n",
    "    model_exec_times.append(0)\n",
    "    learn_times.append(0)\n",
    "    inference_times.append(0)\n",
    "else:\n",
    "    print(f\"Running simulation...\\nResults will be saved to {oksgs_csv}\")\n",
    "    t0 = time.time()\n",
    "    with warnings.catch_warnings():\n",
    "        # Suppress the \"DataFrame is highly fragmented\" warning.\n",
    "        # - This is usually the result of calling `frame.insert` many times,\n",
    "        #   which has poor performance. Consider joining all columns at once\n",
    "        #   using pd.concat(axis=1) instead.\n",
    "        warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "        df_oksgs = KrigingManager.kriging_sequential_simulations(df_domain_bh, df_domain_infer, cfg_oksgs)\n",
    "    model_exec_times.append(time.time() - t0)\n",
    "    learn_times.append(cfg_oksgs['timing:learn'])\n",
    "    inference_times.append(cfg_oksgs['timing:inference'])\n",
    "    print(f\"Writing {oksgs_csv}\")\n",
    "    df_oksgs.to_csv(oksgs_csv)\n",
    "\n",
    "# OK-SGS: Compute mean, stdev using first m simulations\n",
    "max_simul = cfg_krige['simulation:num']\n",
    "two_powers = [2**i for i in np.arange(1, int(np.floor(np.log2(max_simul))+1))]\n",
    "oksgs_mean = {}\n",
    "oksgs_stdev = {}\n",
    "oksgs_sim_cols = [x for x in df_oksgs.columns]\n",
    "oksgs_vals = df_oksgs[oksgs_sim_cols].values\n",
    "for i in two_powers:\n",
    "    oksgs_mean[f'from_{i}'] = np.mean(oksgs_vals[:,:i], axis=1)\n",
    "    oksgs_stdev[f'from_{i}'] = np.std(oksgs_vals[:,:i], axis=1)\n",
    "\n",
    "# Check dictionary contents following simulations, including random states\n",
    "print('{}\\n'.format(cfg_oksgs.keys()))\n",
    "print(f\"simulation:period_domain_id: {cfg_oksgs['simulation:period_domain_id']}\")\n",
    "print(f\"simulation:period_domain_initial_state: {cfg_oksgs['simulation:period_domain_initial_state']}\")\n",
    "print(f\"simulation:path_seeds: {cfg_oksgs['simulation:path_seeds']}\")\n",
    "\n",
    "# Check variogram parameters\n",
    "variogram_props = ['nugget','range','sill','nu','variogram-model','R','S']\n",
    "print(\"kriging:transform_data: {}\".format(cfg_oksgs['kriging:transform_data']))\n",
    "print(\"variogram:params: {}\".format(dict(zip(variogram_props, cfg_oksgs['variogram:params']))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique 2: Ordinary Kriging (without sequential simulation)\n",
    "- Case 'nst': with normal score transformation\n",
    "- Case 'raw': without data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_ok = copy.deepcopy(cfg_krige)\n",
    "cfg_ok['kriging:type'] = 'ordinary_kriging'\n",
    "ok_mean, ok_var = {}, {}\n",
    "ok_col_mean, ok_col_stdev = {}, {}\n",
    "df_ok = pd.DataFrame()\n",
    "ok_abbrev = 'ok' + ('r' if cfg_ok['kriging:transform_data'] else '')\n",
    "ok_csv = f\"{result_path}/predictions-{ok_abbrev}-{domain_id}.csv\"\n",
    "\n",
    "if os.path.isfile(ok_csv):\n",
    "    df_ok = pd.read_csv(ok_csv, index_col=0, header=0)\n",
    "    (ok_mean, ok_var, ok_col_mean, ok_col_stdev) = \\\n",
    "        extract_raw_nst_predictions(df_ok, return_variance=True)\n",
    "    model_names += ['OK_raw', 'OK_nst']\n",
    "    model_exec_times += [0,0]\n",
    "    learn_times += [0,0]\n",
    "    inference_times += [0,0]\n",
    "else:\n",
    "    for xform, bval in [('raw', False), ('nst', True)]:\n",
    "        cfg_ok['kriging:apply_normal_score_transform'] = bval\n",
    "        t0 = time.time()\n",
    "        ok_mean[xform], ok_var[xform] = \\\n",
    "            KrigingManager.kriging_regression(df_domain_bh, df_domain_infer, cfg_ok)\n",
    "        model_names.append(f\"OK_{xform}\")\n",
    "        model_exec_times.append(time.time() - t0)\n",
    "        learn_times.append(cfg_ok['timing:learn'])\n",
    "        inference_times.append(cfg_ok['timing:inference'])\n",
    "        ok_col_mean[xform] = 'ok_mean' + ('_nst' if xform == 'nst' else '')\n",
    "        ok_col_stdev[xform] = 'ok_stdev' + ('_nst' if xform == 'nst' else '')\n",
    "        df_ok[ok_col_mean[xform]] = ok_mean[xform]\n",
    "        df_ok[ok_col_stdev[xform]] = np.sqrt(np.maximum(ok_var[xform], 0))\n",
    "\n",
    "    df_ok = df_ok.set_index(df_domain_infer.index) #pd>Index generally not contiguous\n",
    "    df_ok.to_csv(ok_csv)\n",
    "    print(f\"Writing {ok_csv}\")\n",
    "\n",
    "for xform in ['raw', 'nst']:\n",
    "    print('OK MSE[{}]: {}'.format(xform, np.mean((ok_mean[xform] - ground_truth)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique 3: SK Sequential Gaussian Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_krige['kriging:type'] = 'simple_kriging'\n",
    "cfg_sksgs = copy.deepcopy(cfg_krige)\n",
    "cfg_sksgs['bypass_simulation'] = False\n",
    "sksgs_abbrev = ''.join([x[0] for x in cfg_sksgs['kriging:type'].split('_')]) + 'sgs'\n",
    "sksgs_abbrev += 'r' if cfg_sksgs['kriging:transform_data'] else ''\n",
    "sksgs_csv = f\"{result_path}/predictions-{sksgs_abbrev}-{domain_id}.csv\"\n",
    "\n",
    "model_names.append('SK-SGS')\n",
    "if os.path.isfile(sksgs_csv):\n",
    "    print(f\"Reading {sksgs_csv}\")\n",
    "    cfg_sksgs['bypass_simulation'] = True\n",
    "    KrigingManager.kriging_sequential_simulations(df_domain_bh, df_domain_infer, cfg_sksgs)\n",
    "    df_sksgs = pd.read_csv(sksgs_csv, index_col=0, header=0)\n",
    "    model_exec_times.append(0)\n",
    "    learn_times.append(0)\n",
    "    inference_times.append(0)\n",
    "else:\n",
    "    print(f\"Running simulation...\\nResults will be saved to {sksgs_csv}\")\n",
    "    t0 = time.time()\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "        df_sksgs = KrigingManager.kriging_sequential_simulations(df_domain_bh, df_domain_infer, cfg_sksgs)\n",
    "    model_exec_times.append(time.time() - t0)\n",
    "    learn_times.append(cfg_sksgs['timing:learn'])\n",
    "    inference_times.append(cfg_sksgs['timing:inference'])\n",
    "    print(f\"Writing {sksgs_csv}\")\n",
    "    df_sksgs.to_csv(sksgs_csv)\n",
    "\n",
    "sksgs_mean = {}\n",
    "sksgs_stdev = {}\n",
    "sksgs_sim_cols = [x for x in df_sksgs.columns]\n",
    "sksgs_vals = df_sksgs[sksgs_sim_cols].values\n",
    "for i in two_powers:\n",
    "    sksgs_mean[f'from_{i}'] = np.mean(sksgs_vals[:,:i], axis=1)\n",
    "    sksgs_stdev[f'from_{i}'] = np.std(sksgs_vals[:,:i], axis=1)\n",
    "\n",
    "print(\"kriging:transform_data: {}\".format(cfg_sksgs['kriging:transform_data']))\n",
    "print(\"variogram:params: {}\".format(dict(zip(variogram_props, cfg_sksgs['variogram:params']))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique 4: Simple Kriging (without sequential simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_sk = copy.deepcopy(cfg_krige)\n",
    "cfg_sk['kriging:type'] = 'simple_kriging'\n",
    "sk_mean, sk_var = {}, {}\n",
    "sk_col_mean, sk_col_stdev = {}, {}\n",
    "df_sk = pd.DataFrame()\n",
    "sk_abbrev = 'sk' + ('r' if cfg_sk['kriging:transform_data'] else '')\n",
    "sk_csv = f\"{result_path}/predictions-{sk_abbrev}-{domain_id}.csv\"\n",
    "\n",
    "if os.path.isfile(sk_csv):\n",
    "    df_sk = pd.read_csv(sk_csv, index_col=0, header=0)\n",
    "    (sk_mean, sk_var, sk_col_mean, sk_col_stdev) = \\\n",
    "        extract_raw_nst_predictions(df_sk, return_variance=True)\n",
    "    model_names += ['SK_raw', 'SK_nst']\n",
    "    model_exec_times += [0,0]\n",
    "    learn_times += [0,0]\n",
    "    inference_times += [0,0]\n",
    "else:\n",
    "    for xform, bval in [('raw', False), ('nst', True)]:\n",
    "        cfg_sk['kriging:apply_normal_score_transform'] = bval\n",
    "        t0 = time.time()\n",
    "        sk_mean[xform], sk_var[xform] = \\\n",
    "            KrigingManager.kriging_regression(df_domain_bh, df_domain_infer, cfg_sk)\n",
    "        model_names.append(f\"SK_{xform}\")\n",
    "        model_exec_times.append(time.time() - t0)\n",
    "        learn_times.append(cfg_sk['timing:learn'])\n",
    "        inference_times.append(cfg_sk['timing:inference'])\n",
    "        sk_col_mean[xform] = 'sk_mean' + ('_nst' if xform == 'nst' else '')\n",
    "        sk_col_stdev[xform] = 'sk_stdev' + ('_nst' if xform == 'nst' else '')\n",
    "        df_sk[sk_col_mean[xform]] = sk_mean[xform]\n",
    "        df_sk[sk_col_stdev[xform]] = np.sqrt(np.maximum(sk_var[xform], 0))\n",
    "\n",
    "    df_sk = df_sk.set_index(df_domain_infer.index)\n",
    "    df_sk.to_csv(sk_csv)\n",
    "    print(f\"Writing {sk_csv}\")\n",
    "\n",
    "for xform in ['raw', 'nst']:\n",
    "    print('SK MSE[{}]: {}'.format(xform, np.mean((sk_mean[xform] - ground_truth)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Gaussian Process and simulation parameters\n",
    "- Refer to doc string in `GPManager.gaussian_process_simulations`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_gp = create_gaussian_process_config(specs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique 5: Gaussian Process Regression - Local Mean Approach\n",
    "- GPR Case 1: with normal score transformation\n",
    "- GPR Case 2: without data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'GPR(L)'\n",
    "gpl_mean, gpl_stdev = {}, {}\n",
    "gpl_col_mean, gpl_col_stdev = {}, {}\n",
    "df_gpl = pd.DataFrame()\n",
    "gpl_abbrev = 'gpl' + ('r' if cfg_gp['gp:learning_inference_in_rotated_space'] else '')\n",
    "gpl_csv = f\"{result_path}/predictions-{gpl_abbrev}-{domain_id}.csv\"\n",
    "\n",
    "if os.path.isfile(gpl_csv):\n",
    "    print(f\"Reading {gpl_csv}\")\n",
    "    df_gpl = pd.read_csv(gpl_csv, index_col=0, header=0)\n",
    "    (gpl_mean, gpl_stdev, gpl_col_mean, gpl_col_stdev) = \\\n",
    "        extract_raw_nst_predictions(df_gpl, return_variance=False)\n",
    "    model_names += ['GP(L)_raw', 'GP(L)_nst']\n",
    "    model_exec_times += [0,0]\n",
    "    learn_times += [0,0]\n",
    "    inference_times += [0,0]\n",
    "else:\n",
    "    for xform, bval in [('raw', False), ('nst', True)]:\n",
    "        cfg_gp['gp:apply_normal_score_transform'] = bval\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "            t0 = time.time()\n",
    "            # perform GPR\n",
    "            gpl_mean[xform], gpl_stdev[xform] = \\\n",
    "                GPManager.gaussian_process_simulations(\n",
    "                method, df_domain_bh, df_domain_x, df_domain_infer, cfg_gp)\n",
    "            model_names.append(f\"GP(L)_{xform}\")\n",
    "            model_exec_times.append(time.time() - t0)\n",
    "            learn_times.append(cfg_gp['timing:learn'])\n",
    "            inference_times.append(cfg_gp['timing:inference'])\n",
    "            gpl_col_mean[xform] = cfg_gp['gp:mean_col_name']\n",
    "            gpl_col_stdev[xform] = cfg_gp['gp:stdev_col_name']\n",
    "            df_gpl[gpl_col_mean[xform]] = gpl_mean[xform]\n",
    "            df_gpl[gpl_col_stdev[xform]] = gpl_stdev[xform]\n",
    "\n",
    "    df_gpl = df_gpl.set_index(df_domain_infer.index)\n",
    "    df_gpl.to_csv(gpl_csv)\n",
    "    print(f\"Writing {gpl_csv}\")\n",
    "\n",
    "for xform in ['raw', 'nst']:\n",
    "    print('GPR(L) MSE[{}]: {}'.format(xform, np.mean((gpl_mean[xform] - ground_truth)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique 6: GP Sequential Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "method = 'GP-SGS'\n",
    "cfg_gp['simulation:bypass'] = False\n",
    "gpsgs_abbrev = 'gpsgs' + ('r' if cfg_gp['gp:learning_inference_in_rotated_space'] else '')\n",
    "gpsgs_csv = f\"{result_path}/predictions-{gpsgs_abbrev}-{domain_id}.csv\"\n",
    "\n",
    "model_names.append('GP-SGS')\n",
    "if os.path.isfile(gpsgs_csv):\n",
    "    print(f\"Reading {gpsgs_csv}\")\n",
    "    df_gplsgs = pd.read_csv(gpsgs_csv, index_col=0, header=0)\n",
    "    cfg_gp['simulation:column_names'] = list(df_gplsgs.columns)\n",
    "    cfg_gp['simulation:bypass'] = True\n",
    "    GPManager.gaussian_process_simulations(\n",
    "        method, df_domain_bh, df_domain_x, df_domain_infer, cfg_gp)\n",
    "    model_exec_times.append(0)\n",
    "    learn_times.append(0)\n",
    "    inference_times.append(0)\n",
    "else:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "        print(f\"Running simulation...\\nResults will be saved to {gpsgs_csv}\")\n",
    "        t0 = time.time()\n",
    "        df_gplsgs = GPManager.gaussian_process_simulations(\n",
    "                   method, df_domain_bh, df_domain_x, df_domain_infer, cfg_gp)\n",
    "        model_exec_times.append(time.time() - t0)\n",
    "        learn_times.append(cfg_gp['timing:learn'])\n",
    "        inference_times.append(cfg_gp['timing:inference'])\n",
    "        print(f\"Writing {gpsgs_csv}\")\n",
    "        df_gplsgs.to_csv(gpsgs_csv)\n",
    "\n",
    "gpsgs_mean = {}\n",
    "gpsgs_stdev = {}\n",
    "gpsgs_vals = df_gplsgs[cfg_gp['simulation:column_names']].values\n",
    "\n",
    "max_simul = cfg_gp['simulation:num']\n",
    "two_powers = [2**i for i in np.arange(1, int(np.floor(np.log2(max_simul))+1))]\n",
    "\n",
    "for i in two_powers:\n",
    "    gpsgs_mean[f'from_{i}'] = np.mean(gpsgs_vals[:,:i], axis=1)\n",
    "    gpsgs_stdev[f'from_{i}'] = np.std(gpsgs_vals[:,:i], axis=1)\n",
    "\n",
    "min_vsd = min([min(s) for s in gpsgs_stdev.values()])\n",
    "max_vsd = max([max(s) for s in gpsgs_stdev.values()])\n",
    "\n",
    "# Check dictionary contents following simulations, including random states\n",
    "print('{}\\n'.format(cfg_gp.keys()))\n",
    "print(f\"simulation:period_domain_id: {cfg_gp['simulation:period_domain_id']}\")\n",
    "print(f\"simulation:period_domain_initial_state: {cfg_gp['simulation:period_domain_initial_state']}\")\n",
    "print(f\"simulation:path_seeds: {cfg_gp['simulation:path_seeds']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique 7: Gaussian Process Regression - Global Mean Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'GPR(G)'\n",
    "gpg_mean, gpg_stdev = {}, {}\n",
    "gpg_col_mean, gpg_col_stdev = {}, {}\n",
    "df_gpg = pd.DataFrame()\n",
    "gpg_abbrev = 'gpg' + ('r' if cfg_gp['gp:learning_inference_in_rotated_space'] else '')\n",
    "gpg_csv = f\"{result_path}/predictions-{gpg_abbrev}-{domain_id}.csv\"\n",
    "\n",
    "if os.path.isfile(gpg_csv):\n",
    "    print(f\"Reading {gpg_csv}\")\n",
    "    df_gpg = pd.read_csv(gpg_csv, index_col=0, header=0)\n",
    "    (gpg_mean, gpg_stdev, gpg_col_mean, gpg_col_stdev) = \\\n",
    "        extract_raw_nst_predictions(df_gpg, return_variance=False)\n",
    "    model_names += ['GP(G)_raw', 'GP(G)_nst']\n",
    "    model_exec_times += [0,0]\n",
    "    learn_times += [0,0]\n",
    "    inference_times += [0,0]\n",
    "else:\n",
    "    for xform, bval in [('raw', False), ('nst', True)]:\n",
    "        cfg_gp['gp:apply_normal_score_transform'] = bval\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "            t0 = time.time()\n",
    "            # perform GPR\n",
    "            gpg_mean[xform], gpg_stdev[xform] = \\\n",
    "                GPManager.gaussian_process_simulations(\n",
    "                method, df_domain_bh, df_domain_x, df_domain_infer, cfg_gp)\n",
    "            model_names.append(f\"GP(G)_{xform}\")\n",
    "            model_exec_times.append(time.time() - t0)\n",
    "            learn_times.append(cfg_gp['timing:learn'])\n",
    "            inference_times.append(cfg_gp['timing:inference'])\n",
    "            gpg_col_mean[xform] = cfg_gp['gp:mean_col_name']\n",
    "            gpg_col_stdev[xform] = cfg_gp['gp:stdev_col_name']\n",
    "            df_gpg[gpg_col_mean[xform]] = gpg_mean[xform]\n",
    "            df_gpg[gpg_col_stdev[xform]] = gpg_stdev[xform]\n",
    "\n",
    "    df_gpg = df_gpg.set_index(df_domain_infer.index)\n",
    "    df_gpg.to_csv(gpg_csv)\n",
    "    print(f\"Writing {gpg_csv}\")\n",
    "\n",
    "for xform in ['raw', 'nst']:\n",
    "    print('GPR(G) MSE[{}]: {}'.format(xform, np.mean((gpg_mean[xform] - ground_truth)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique 8: GP Spatially Correlated (or Cholesky) Random Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "method = 'GP-CRF'\n",
    "gpcrf_abbrev = 'gpcrf' + ('r' if cfg_gp['gp:learning_inference_in_rotated_space'] else '')\n",
    "gpcrf_csv = f\"{result_path}/predictions-{gpcrf_abbrev}-{domain_id}.csv\"\n",
    "\n",
    "model_names.append('GP-CRF')\n",
    "if os.path.isfile(gpcrf_csv):\n",
    "    print(f\"Reading {gpcrf_csv}\")\n",
    "    df_gplcrf = pd.read_csv(gpcrf_csv, index_col=0, header=0)\n",
    "    cfg_gp['simulation:column_names'] = list(df_gplcrf.columns)\n",
    "    cfg_gp['simulation:bypass'] = True\n",
    "    GPManager.gaussian_process_simulations(method, df_domain_bh, df_domain_x, df_domain_infer, cfg_gp)\n",
    "    model_exec_times.append(0)\n",
    "    learn_times.append(0)\n",
    "    inference_times.append(0)\n",
    "else:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "        print(f\"Running simulation...\\nResults will be saved to {gpcrf_csv}\")\n",
    "        t0 = time.time()\n",
    "        df_gplcrf = GPManager.gaussian_process_simulations(\n",
    "                   method, df_domain_bh, df_domain_x, df_domain_infer, cfg_gp)\n",
    "        model_exec_times.append(time.time() - t0)\n",
    "        learn_times.append(cfg_gp['timing:learn'])\n",
    "        inference_times.append(cfg_gp['timing:inference'])\n",
    "        print(f\"Writing {gpcrf_csv}\")\n",
    "        df_gplcrf.to_csv(gpcrf_csv)\n",
    "\n",
    "gpcrf_mean = {}\n",
    "gpcrf_stdev = {}\n",
    "gpcrf_vals = df_gplcrf[cfg_gp['simulation:column_names']].values\n",
    "for i in two_powers:\n",
    "    gpcrf_mean[f'from_{i}'] = np.mean(gpcrf_vals[:,:i], axis=1)\n",
    "    gpcrf_stdev[f'from_{i}'] = np.std(gpcrf_vals[:,:i], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record execution times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = len(model_names)\n",
    "df_t = pd.DataFrame({'inference_period': [mA] * num,\n",
    "                     'domain_id': [domain_id] * num,\n",
    "                     'training_samples': [len(df_domain_bh)] * num,\n",
    "                     'inference_locations': [len(df_domain_infer)] * num,\n",
    "                     'model': model_names,\n",
    "                     'execution_time': model_exec_times,\n",
    "                     'learn_time': learn_times,\n",
    "                     'inference_time': inference_times})\n",
    "timing_csv = f\"{result_path}/timing_{domain_id}.csv\"\n",
    "if not os.path.exists(timing_csv):\n",
    "    df_t.to_csv(timing_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble predictions for all candidate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_mu = OrderedDict(\n",
    "                [('SK', sk_mean['raw']), ('SK_nst', sk_mean['nst']),\n",
    "                 ('OK', ok_mean['raw']), ('OK_nst', ok_mean['nst']),\n",
    "                 ('GP(L)', df_gpl[gpl_col_mean['raw']].values),\n",
    "                 ('GP(L)_nst', df_gpl[gpl_col_mean['nst']].values),\n",
    "                 ('GP(G)', df_gpg[gpg_col_mean['raw']].values),\n",
    "                 ('GP(G)_nst', df_gpg[gpg_col_mean['nst']].values)\n",
    "                ])\n",
    "candidates_sigma = OrderedDict(\n",
    "                   [('SK', np.sqrt(sk_var['raw'])), ('SK_nst', np.sqrt(sk_var['nst'])),\n",
    "                    ('OK', np.sqrt(ok_var['raw'])), ('OK_nst', np.sqrt(ok_var['nst'])),\n",
    "                    ('GP(L)', df_gpl[gpl_col_stdev['raw']].values),\n",
    "                    ('GP(L)_nst', df_gpl[gpl_col_stdev['nst']].values),\n",
    "                    ('GP(G)', df_gpg[gpg_col_stdev['raw']].values),\n",
    "                    ('GP(G)_nst', df_gpg[gpg_col_stdev['nst']].values)\n",
    "                   ])\n",
    "for i in two_powers:\n",
    "    key = f\"from_{i}\"\n",
    "    candidates_mu[f\"SK_SGS_{key}\"] = sksgs_mean[key]\n",
    "    candidates_sigma[f\"SK_SGS_{key}\"] = sksgs_stdev[key]\n",
    "for i in two_powers:\n",
    "    key = f\"from_{i}\"\n",
    "    candidates_mu[f\"OK_SGS_{key}\"] = oksgs_mean[key]\n",
    "    candidates_sigma[f\"OK_SGS_{key}\"] = oksgs_stdev[key]\n",
    "for i in two_powers:\n",
    "    key = f\"from_{i}\"\n",
    "    candidates_mu[f\"GP_SGS_{key}\"] = gpsgs_mean[key]\n",
    "    candidates_sigma[f\"GP_SGS_{key}\"] = gpsgs_stdev[key]\n",
    "for i in two_powers:\n",
    "    key = f\"from_{i}\"\n",
    "    candidates_mu[f\"GP_CRF_{key}\"] = gpcrf_mean[key]\n",
    "    candidates_sigma[f\"GP_CRF_{key}\"] = gpcrf_stdev[key]\n",
    "\n",
    "# Write results to pickle file\n",
    "for k in ['timing:learn', 'timing:inference']:\n",
    "    _ = cfg_krige.pop(k, None)\n",
    "    _ = cfg_gp.pop(k, None)\n",
    "\n",
    "models_pfile = f\"{result_path}/models-{domain_id}.p\"\n",
    "with open(models_pfile, 'wb') as hdl:\n",
    "    variables = [candidates_mu, candidates_sigma, cfg_krige, cfg_gp]\n",
    "    pickle.dump(variables, hdl, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#cc0066\">Part 2: Models Evaluation</font>\n",
    "A _model_ refers to a probabilistic grade prediction technique such as Ordinary Kriging, Gaussian Process Regresion, and any sequential or random field simulation approaches such as OK-SGS, GP-SGS and GP-CRF. \n",
    "\n",
    "### Objective\n",
    "For each of the model considered, we would like to\n",
    "- 1) Compute <b>histogram distances</b> between the model <font color=\"#4F81FF\"><b>mean prediction</b></font> and ground truth grade values\n",
    "- 2) Compute <b>variogram ratios</b> and spatial variability of the model mean prediction relative to the ground truth\n",
    "- 3) Compute the <b>goodness of model</b> <font color=\"#4F81FF\"><b>uncertainty estimates</b></font>\n",
    "\n",
    "These statistical measures are described below.\n",
    "\n",
    "### <font color=\"#EE0066\">2.1. Measuring discrepancies between model prediction and ground truth histograms</font>\n",
    "Let $p$ and $q$ denote the probability mass function of the model predicted mean grade and ground truth, respectively. Let us consider four measures motivated by hypothesis testing, information theory, set theory and the Monge-Kantorovich optimal transportation (distribution morphing) problem.\n",
    "\n",
    "#### Probabilistic symmetric Chi-square measure (Deza, 2009)\n",
    "This represents twice the <i>triangular discrimination</i> defined in (Topsoe, 2000).\n",
    "- $D_{psChi} = 2 \\sum_x \\dfrac{\\left|p(x)-q(x)\\right|^2}{p(x)+q(x)}$\n",
    "\n",
    "#### Jensen-Shannon (JS) divergence\n",
    "The Jensen-Shannon divergence measure, $D_{JS}$, represents the symmetric form of K divergence. It is defined by\n",
    "- $\\begin{align}\\\\\n",
    "   D_{JS} &= \\dfrac{1}{2} \\left[ \\sum_x p(x) \\log\\left(\\dfrac{2 p(x)}{p(x)+q(x)}\\right) + \\sum_x q(x) \\log\\left(\\dfrac{2 q(x)}{p(x)+q(x)}\\right)\\right]\\\\\n",
    "          &= \\dfrac{1}{2} [KL(p|m) + KL(q|m)]\\end{align}$\n",
    "\n",
    "In the second line, $m=(p+q)/2$. It expresses $D_{JS}$ in terms of the Kullback-Leibler distance $KL(p\\mid q) = \\sum_x p(x) log(p(x)/q(x))$. Using base-2 logarithm, this measure satisfies the bounds $0 \\le D_{JS} \\le 1$, attaining zero when $p = q$.\n",
    "\n",
    "#### Ruzicka distance\n",
    "The Ruzicka distance $D_{Ruz}$ is defined by Ruzicka similarity $S_{Ruz}$. Given two probability mass functions, $p$ and $q$,\n",
    "- $D_{Ruz} = 1 - S_{Ruz} = 1 - \\dfrac{\\sum_x \\min\\{p(x),q(x)\\}}{\\sum_x \\max\\{p(x),q(x)\\}}$\n",
    "\n",
    "This may be interpreted as the `intersection(p,q)` over `union(p,q)` as a generalisation of the Jaccard similarity index from $\\{0,1\\}^n$ to $\\mathbb{R}^n$. Ruzicka distance is bounded by $0 \\le D_{Ruz} \\le 1$.\n",
    "\n",
    "#### Wasserstein distance\n",
    "The Wasserstein distance $W_1$, also called the <b>Earth mover’s distance</b> or the Kantorovich <b>optimal transport distance</b>, is a similarity metric that may be interpreted as the minimum energy cost of moving and transforming a pile of dirt in the shape of one probability distribution into the other. The cost is quantified by the distance and amount of probability mass being moved. It might be preferred over JS divergence, as the Kantorovich-Mallows-Monge-Wasserstein metric represents the Lipschitz distance between probability measures and has to be K-Lipschitz continuous. When the measures are uniform over a set of discrete elements, the problem is also known as minimum weight bipartite matching.\n",
    "\n",
    "Formally, the $p$-Wasserstein distance between probability distributions $P$ and $Q$ can be <a href=\"https://en.m.wikipedia.org/wiki/Earth_mover's_distance\">defined</a> as an infinum over joint probabilities\n",
    "- $D_{EM}(P,Q) \\equiv W_p(P,Q) = \\inf_{\\gamma\\in\\Pi(P,Q)} \\mathbb{E}_{(x,y)\\sim\\gamma}[d(x,y)]$\n",
    "\n",
    "where $\\Pi(P,Q)$ is the set of all joint distributions whose marginals are $P$ and $Q$. In general, it requires solving a <a href=\"https://en.wikipedia.org/wiki/Linear_assignment_problem\">linear assignment problem</a>. However, in one-dimension, it can be <a href=\"https://en.wikipedia.org/wiki/Wasserstein_metric\">obtained</a> simply using <b>order statistics</b>. In particular, $W_p(P,Q)=\\left(\\frac{1}{n}\\sum_i^{n}\\lVert X_{(i)}-Y_{(i)}\\rVert^p\\right)^{1/p}$. Observe that this does not require instances (predicted or true values) to be quantised or converted into histograms.\n",
    "\n",
    "### <font color=\"#EE0066\">2.2. Measuring loss of spatial fidelity using semi-variograms</font>\n",
    "Variograms are useful for understanding spatial variability, viz., the correlation between points in space as a function of their separating distance. When the amplitude of the sill (height of the semi-variogram at large distances as samples become uncorrelated) decreases relative to a benchmark (typically, the variogram based on validation measurements at the inference locations in the block model, or training blastholes), smoothing is said to have occurred. Thus, the ratio between two variogram curves, with one being chosen as the benchmark, indicates power attenuation or a loss in spatial fidelity. Accordingly, L-statistics (such as the median and lower/upper quartiles) can be extracted from these variogram curve ratios, converting a visual diagnostic tool into a quantitative measure as shown below.\n",
    "- Variogram ratios as a function of lag distance $x$:\n",
    "  - $r_\\text{method}(x) = \\dfrac{\\gamma_\\text{method}(x)}{\\gamma_\\text{reference}(x)}$\n",
    "- Median variogram ratios and lower/upper quartiles:\n",
    "  - $\\text{median}_x(r_\\text{method}(x))$, percentile($r_\\text{method}(x), [25,75])$\n",
    "\n",
    "### <font color=\"#EE0066\">2.3. Measuring goodness of model uncertainty estimates</font>\n",
    "\n",
    "Measures to be described\n",
    "- <b>Signed scoring function</b>, $S\\equiv s(\\hat{\\mu},\\hat{\\sigma}\\mid\\mu_0)$,\n",
    "- <b>Local Consensus</b> (a reasonableness measure) $L\\equiv l(\\hat{\\mu},\\hat{\\sigma}\\mid\\mu_0)=\\left|s \\right|$\n",
    "- <b>Fraction of true values contained in $p$-probability intervals</b> bounded by quantiles $\\left[Q_{(1-p)/2},Q_{(1+p)/2}\\right]$, $\\bar{\\kappa}(p)$\n",
    "- <b>Accuracy</b> of the conditional distribution function, $A$, averaged over $p$\n",
    "- <b>Precision</b> of the conditional distribution function, $P$, averaged over $p$\n",
    "- <b>Goodness</b> statistic (Deutsch, 1997), $G$\n",
    "- <b>Tightness</b> statistic, $T$, based on a slight modification of (Goovaerts, 2001).\n",
    "\n",
    "Each model will provide a mean estimate $\\hat{\\mu}_j$ and standard deviation estimate $\\hat{\\sigma}_j$ at inference location $X_j$ where a validation measurement (true grade) $\\mu_{0,j}\\equiv Y_j$ is provided. Here, $(\\hat{\\mu}_j,\\hat{\\sigma}_j)$ are deemed to be sufficient for characterising the random process or conditional distribution function (cdf) which will be assumed as Gaussian distributed. For brevity, the location subscript $j$ may be dropped in subsequent discussion.\n",
    "\n",
    "It is common to convert $(\\mu_{0,j},\\hat{\\mu}_j,\\hat{\\sigma}_j)$ into a Z score,\n",
    "viz $z_j = (\\mu_{0,j}-\\hat{\\mu}_j)\\,/\\,\\hat{\\sigma}_j$. In the following figure, the black dot represents the supposed true value ($\\mu_0$). The first row considers the case where the model mean _underestimates_ the true value ($\\mu_0 > \\hat{\\mu}$). The second row represents the case where the model mean _overestimates_ the true value ($\\mu_0 < \\hat{\\mu}$).\n",
    "\n",
    "The shaded area in the left column represents the <font color=\"#cc0066\">coverage probability</font>, denoted $p$. To obtain the <font color=\"#cc0066\">local consensus</font> between model prediction and the groundtruth, we will generally be interested in its complement $1-p$ which corresponds to the tail section (shaded portion under the curve) in the right column. To distinguish overestimation from underestimation, it is convenient to introduce a signed scoring function called <font color=\"#cc0066\">synchronicity</font>\n",
    "- $s(\\hat{\\mu},\\hat{\\sigma}\\mid \\mu_0) \\equiv s = \\begin{cases}2\\times\\left[1 - \\Phi(z)\\right] & \\text{if }\\mu_0 \\ge \\hat{\\mu}\\\\- 2 \\Phi(z) & \\text{otherwise}\\\\ \\end{cases}$\n",
    "- where $z=(\\mu_0 - \\hat{\\mu})/\\hat{\\sigma}$ and $\\Phi(z) = \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{z} e^{-t^2/2} dt$ denotes the CDF of the standard normal distribution.\n",
    "\n",
    "This may be implemented neatly using an indicator function `under_estimated = mu_hat < mu_0` so that\n",
    "- `s = 2 * (under_estimated * (1 - norm.cdf(z_scores)) - (1 - under_estimated) * norm.cdf(z_scores))`\n",
    "<div class=\"alert alert-box alert-warning\">\n",
    "Using the identities $\\Phi(z)=\\frac{1}{2}(1+\\text{erf}(\\frac{z}{\\sqrt{2}}))$ and $\\Phi_c(z)=1-\\Phi(z)=\\frac{1}{2}\\text{erfc}(\\frac{z}{\\sqrt{2}})$,<br>where $\\text{erf}(z)=\\frac{2}{\\sqrt{\\pi}}\\int_{0}^{z} e^{-t^2}dt$ and $\\text{erfc}(z)=\\frac{2}{\\sqrt{\\pi}}\\int_{z}^{\\infty} e^{-t^2}dt$,<br>we can also write \n",
    "    $s(\\hat{\\mu},\\hat{\\sigma}\\mid \\mu_0) \\equiv s = \\begin{cases}\\text{erfc}(\\frac{z}{\\sqrt{2}}) & \\text{if }\\mu_0 \\ge \\hat{\\mu}\\\\- (1+\\text{erf}(\\frac{z}{\\sqrt{2}})) & \\text{otherwise}\\\\ \\end{cases}$\n",
    "</div>\n",
    "\n",
    "The local consensus term is simply given by $l(\\hat{\\mu},\\hat{\\sigma}\\mid \\mu_0) = \\left|s\\right|$\n",
    "- When $\\hat{\\mu} = \\mu_0$, the local consensus $l(\\hat{\\mu},\\hat{\\sigma}\\mid \\mu_0) = 1$\n",
    "- In the limit as $\\hat{\\mu} - \\mu_0 \\rightarrow \\pm\\infty$, the local consensus $l(\\hat{\\mu},\\hat{\\sigma}\\mid \\mu_0) \\rightarrow 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=os.path.join(default_data_dir, \"gaussian-uncertainty-plot.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connections with statistical measures described in the literature\n",
    "\n",
    "#### Quality of the predicted mean \n",
    "Accuracy of the predicted mean can be measured using the mean squared error metric $MSE = \\frac{1}{m}\\sum_{j=1}^{m}\\left|\\hat{\\mu} - \\mu_0\\right|^2$. However, this does not take into account any spatial correlation. In this work, we use <b>variogram ratios</b> to quantify the spatial variability observed in a model relative to a ground truth (based on validation measurements or training samples).\n",
    "- Attention on the predicted mean and variogram ratios represents a crucial part of our model fidelity assessment. Nevertheless, we will be skipping over this to focus on the uncertainty aspect in our present discussion. Interested readers can refer to the description of variogram ratios near cell 19 in Notebook 2E to get the gist of it.\n",
    "\n",
    "#### Goodness of the model predicted uncertainty\n",
    "One criterion for assessing <b>prediction uncertainty accuracy</b> (Fouedjio and Klump, 2019) is based on the <font color=\"#cc0066\">goodness statistic</font> (Deutsch, 1997). By construction, there is a probability $p$ (the same as the coverage probability $p$ in our earlier formulation) that the true value of the random variable falls inside the <b>symmetric p-probability interval</b> (PI) bounded by the $p_L=(1-p)/2$ and $p_U=(1+p)/2$ <b>quantiles</b> ($Q_L\\equiv Q_{(1-p)/2}$ and $Q_U\\equiv Q_{(1+p)/2}$) of the estimated conditional distribution function.\n",
    "- As a special case, when $p=0.5$, $Q_L\\equiv Q_{0.25}$ and $Q_U\\equiv Q_{0.75}$ correspond to the lower and quartiles, respectively.\n",
    "\n",
    "Given validation measurements $\\{Y_j\\}_{j=1,..,m}$ (or true grades $\\{\\mu_{0,j}\\}_j$) at inference locations $\\{X^{*}_j\\}_j$, we are interested in the <b>fraction of true values that are bounded by the PI interval with various probability</b> $p$. Concretely,\n",
    "- $\\bar{\\kappa}(p) = \\frac{1}{m}\\sum_{j=1}^m \\kappa_j(p)$ defines the expectation over the locations, with\n",
    "- $\\kappa_j(p) =\\begin{cases}1 & \\text{if }\\hat{Q}_{(1-p)/2}(j) < Y_j < \\hat{Q}_{(1+p)/2}(j)\\\\0 & \\text{otherwise}\\\\ \\end{cases}$\n",
    "\n",
    "In practice, when the random process (fluctuations about the posterior mean function) is modelled as Gaussian, we have symmetric intervals following Z-score transformation, so effectively $\\hat{Q}_{(1-p)/2}(j)=-\\hat{Q}_{(1+p)/2}(j)$. The mean proportion $K$ is computed by integrating $\\bar{\\kappa}(p)$\n",
    "- $K = \\int_{0}^{1} \\bar{\\kappa}(p) dp$\n",
    "\n",
    "#### Accuracy of the estimated distribution\n",
    "The average of $[\\bar{\\kappa}(p)\\ge p]$ over $p$ is known as distribution accuracy.\n",
    "- $A = \\int_{0}^{1} \\mathbb{I}(\\bar{\\kappa}(p)) dp$\n",
    "- The indicator function $\\mathbb{I}(\\bar{\\kappa}(p))\\equiv\\mathbb{I}(p)=\\begin{cases}1 & \\text{if }\\bar{\\kappa}(p) \\ge p\\\\0 & \\text{otherwise}\\end{cases}$\n",
    "\n",
    "#### Precision of the estimated distribution\n",
    "Precision measures the narrowness of the model estimated distribution. It is only defined for accurate probability distributions. A $p$-probability interval that _recalls_ more than $p\\,$\\% of true values is accurate but not precise. Optimal precision means the $p$-PI contains the true values exactly $p\\,$\\% of time. On this basis, the precision of the estimated distribution when there is accuracy is defined in (<a href=\"#deutsch1997\">Deutsch, 1997</a>) by\n",
    "- $P = 1 - 2\\int_{0}^{1} \\mathbb{I}(p)\\left[\\bar{\\kappa}(p)-p\\right] dp$\n",
    "<div class=\"alert alert-box alert-info\">This approach relates to y-values in a <a href=\"#prediction-uncertainty-accuracy-plot\">prediction uncertainty accuracy plot</a>. Once again, the precision of the estimated distribution is only meaningful when we have an accurate model, one where the estimated proportions $\\bar{\\kappa}(p)$ are consistently above the expected proportions $p$, or one with a high accuracy score ($A$).</div>\n",
    "<div class=\"alert alert-box\">A model with lower accuracy might be more selective (cautious) about the range of values that it includes, so its prediction intervals tend to be shorter. This may lead to higher precision when only samples that meet the $\\mathbb{I}(p)$ criterion are considered, if $\\bar{\\kappa}(p)$ rises above $y=p$ at all.</div>\n",
    "\n",
    "#### Prediction uncertainty accuracy plots<a name=\"prediction-uncertainty-accuracy-plot\"></a>\n",
    "This refers to a scatter plot of the estimated proportion $\\bar{\\kappa}(p)$ vs the expected proportion $p$. The model estimated CDF is considered accurate when $\\bar{\\kappa}(p) > p$ for all $p\\in [0,1]$. <font color=\"#cccccc\">Examples to follow later in this notebook</font>.\n",
    "\n",
    "#### Prediction uncertainty goodness statistic\n",
    "The closeness between the estimated and theoretical proportions is quantified by\n",
    "- $G = 1 - \\int_{0}^{1}\\left[3 \\mathbb{I}(p)-2\\right]\\left[\\bar{\\kappa}(p)-p\\right] dp$ ......(Deutsch, 1997)\n",
    "\n",
    "The $G$-statistic corresponds to the closeness of points to the bisector of the accuracy plot. Unlike the accuracy and precision, this also considers instances where $\\bar{\\kappa}(p) < p$.\n",
    "- $G=1$ when $\\bar{\\kappa}(p) = p$, $\\forall p\\in [0,1]$\n",
    "- $G=0$ when none of the true values are contained in any PIs\n",
    "- The choice of weights (-2 vs +1) indicates that $\\bar{\\kappa}(p) < p$ is more consequential. The penalty for <i>below the expected</i> proportions ($\\bar{\\kappa}(p) < p$) is twice that for <i>above the expected</i> proportions ($\\bar{\\kappa}(p) > p$).\n",
    "\n",
    "#### Width of prediction uncertainty\n",
    "For models with similar goodness statistics, one would prefer a model where the p-probability interval is as narrow as possible. The tighter the intervals, the smaller the uncertainties. A model (or conditional cumulative distribution function) that consistently provides narrow and accurate PIs should be preferred over another that provides wide and accurate PIs. Different notions of spread such as entropy, variance or inter-quartile range can be used. <a href=\"#goovaerts2002\">Goovaerts (2001)</a> proposed using the interval width to measure the average <font color=\"#cc0066\">tightness of the p-probability intervals subject to containment of the true value</font>\n",
    "- $\\bar{W}(p) = \\dfrac{1}{m \\bar{\\kappa}(p)}\\sum_{j=1}^{m} \\kappa_j(p)\\left[\\hat{Q}_{(1+p)/2}(j)-\\hat{Q}_{(1-p)/2}(j)\\right]$\n",
    "\n",
    "#### Prediction uncertainty tightness statistic\n",
    "At RTCMA, we define this as an average of $\\bar{W}(p)$ over $p$ in a manner similar to the accuracy statistic. To make the tightness scale more meaningful, the average uncertainty interval is normalised by the process standard deviation $\\sigma_Y$ observed in the validation data.\n",
    "- $T = \\dfrac{1}{\\sigma_Y} \\int_{0}^{1} \\bar{W}(p) dp$\n",
    "\n",
    "In general, both $G$ and $T$ need to be taken in account when assessing probabilistic models. \"Uncertainty cannot be artificially reduced at the expense of accuracy\" (Deutsch, 1997)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the histogram, variogram and uncertainty-based performance measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General setup\n",
    "mA = inference_prefix\n",
    "mA_mB_mC = '%02d_%02d_%02d' % (mA, mA+1, mA+2)\n",
    "default_code_dir = os.getcwd()\n",
    "default_data_dir = default_code_dir.replace('code', 'data')\n",
    "default_result_dir = default_code_dir.replace('code', 'results')\n",
    "info_data_dir = specs.get('info:data_dir', default_data_dir)\n",
    "info_result_dir = specs.get('info:result_dir', default_result_dir)\n",
    "data_path = f\"{info_data_dir}/{mA_mB_mC}\"\n",
    "subdir = check_learning_rotation_status(specs)\n",
    "result_path = f\"{info_result_dir}/{subdir}/{mA_mB_mC}\"\n",
    "figure_path = f\"{info_result_dir}/{subdir}/{mA_mB_mC}/figs\"\n",
    "os.makedirs(figure_path, exist_ok=True)\n",
    "specs.update({'mA': mA, 'domain_id': domain_id})\n",
    "\n",
    "models_pfile = f\"{result_path}/models-{domain_id}.p\"\n",
    "histogram_stats_pfile = f\"{result_path}/histogram-stats-{domain_id}.p\"\n",
    "variogram_stats_pfile = f\"{result_path}/variogram-stats-{domain_id}.p\"\n",
    "uncertainty_stats_pfile = f\"{result_path}/uncertainty-stats-{domain_id}.p\"\n",
    "analysis_csv = f\"{result_path}/analysis-{domain_id}.csv\"\n",
    "hdist_crossplots_file = f\"{figure_path}/histogram-dist-crossplots-{domain_id}.pdf\"\n",
    "histogram_file = f\"{figure_path}/histograms-{domain_id}.pdf\"\n",
    "variogram_file = f\"{figure_path}/variograms-@-{domain_id}.pdf\"\n",
    "kappa_w_file = f\"{figure_path}/uncertainty_kappa-{domain_id}.pdf\"\n",
    "spatial_mean_file = f\"{figure_path}/spatial_mean-@-{domain_id}.pdf\"\n",
    "spatial_stdev_file = f\"{figure_path}/spatial_stdev-@-{domain_id}.pdf\"\n",
    "signed_distortion_file = f\"{figure_path}/uncertainty_signed_distortion-{domain_id}.pdf\"\n",
    "local_consensus_file = f\"{figure_path}/local_consensus-{domain_id}.pdf\"\n",
    "\n",
    "if not os.path.exists(models_pfile):\n",
    "    raise RuntimeError(f\"Required file {models_pfile} is missing. You may need to \"\n",
    "                        \"run `construct_models` to generate prediction results\")\n",
    "with open(models_pfile, 'rb') as f:\n",
    "    (candidates_mu, candidates_sigma, cfg_krige, cfg_gp) = pickle.load(f)\n",
    "\n",
    "df_bh = pd.read_csv(f\"{data_path}/blastholes_tagged.csv\")\n",
    "df_bh = df_bh.rename(columns = {'EAST': 'X', 'NORTH': 'Y', 'RL': 'Z', 'PL_CU': 'V'})\n",
    "df_domain_bh = df_bh[(df_bh['lookup_domain'] == domain_id) & np.isfinite(df_bh['V'].values)]\n",
    "df_k = pd.read_csv(f\"{data_path}/blocks_to_estimate_tagged.csv\")\n",
    "df_domain_infer = pd.DataFrame(df_k[df_k['domain'] == domain_id])\n",
    "mu_0 = ground_truth = df_domain_infer['cu_bh_nn'].values\n",
    "max_simul = cfg_krige['simulation:num']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category 1: Histogram measures for mean prediction (global accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(histogram_stats_pfile):\n",
    "    with open(histogram_stats_pfile, 'rb') as f:\n",
    "        (candidates_psChi2, candidates_JS, candidates_IOU, candidates_EM,\n",
    "         candidates_hist, bins_representation, df_h) = pickle.load(f)\n",
    "    stat_names = list(df_h.columns)\n",
    "else:\n",
    "    candidates_psChi2 = OrderedDict()\n",
    "    candidates_JS = OrderedDict()\n",
    "    candidates_IOU = OrderedDict()\n",
    "    candidates_EM = OrderedDict()\n",
    "    candidates_hist = OrderedDict()\n",
    "\n",
    "    for k in candidates_mu.keys():\n",
    "        d = compute_histogram_statistics(mu_0, candidates_mu[k])\n",
    "        candidates_psChi2[k] = d['psym-chi-square']\n",
    "        candidates_JS[k] = d['jensen-shannon']\n",
    "        candidates_IOU[k] = d['ruzicka']\n",
    "        candidates_EM[k] = d['wasserstein']\n",
    "        candidates_hist[k] = d['pmf_x']\n",
    "\n",
    "    bins_representation = d['values']\n",
    "    candidates_hist['GroundTruth'] = d['pmf_y']\n",
    "\n",
    "    # Rank models by histogram distances\n",
    "    compute_rank = lambda seq: np.array([seq[k] for k in candidates_mu.keys()]).argsort().argsort() + 1\n",
    "    rank_psChi2 = compute_rank(candidates_psChi2)\n",
    "    rank_JS = compute_rank(candidates_JS)\n",
    "    rank_IOU = compute_rank(candidates_IOU)\n",
    "    rank_EM = compute_rank(candidates_EM)\n",
    "    rank_mean = np.mean(np.c_[rank_psChi2, rank_JS, rank_IOU, rank_EM], axis=1)\n",
    "    rank_overall = rank_mean.argsort().argsort() + 1\n",
    "\n",
    "    data = OrderedDict()\n",
    "    stat_names = ['d(psChi2)', 'd(JS)', 'd(IOU)', 'd(EM)']\n",
    "    for k in candidates_mu.keys():\n",
    "        data[k] = np.round([candidates_psChi2[k], candidates_JS[k], candidates_IOU[k], candidates_EM[k]], 4)\n",
    "    df_h = pd.DataFrame.from_dict(data, orient='index', columns=stat_names)\n",
    "    df_h['rank(psChi2)'] = rank_psChi2\n",
    "    df_h['rank(JS)'] = rank_JS\n",
    "    df_h['rank(IOU)'] = rank_IOU\n",
    "    df_h['rank(EM)'] = rank_EM\n",
    "    df_h['rank(avg)'] = rank_mean\n",
    "    df_h['rank(overall)'] = rank_overall\n",
    "\n",
    "    with open(histogram_stats_pfile, 'wb') as hdl:\n",
    "        variables = [candidates_psChi2, candidates_JS, candidates_IOU, candidates_EM,\n",
    "                     candidates_hist, bins_representation, df_h]\n",
    "        pickle.dump(variables, hdl, protocol=4)\n",
    "\n",
    "df_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine cross-plots of histogram measures to check consistency\n",
    "Probabilistic Symmetric Chi-square, Jensen-Shannon and Ruzicka.IOU are highly correlated $\\left(\\rho \\gtrsim 0.975\\right)$. Overall, apart from the Wasserstein (Earth Mover's) distance which is slightly different (although not necessarily inferior) to others, they exhibit a monotonic trend.\n",
    "- JS divergence may be used as a substitute for  p.s. Chi-square. A nice property is that it is upper bounded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8)) #(x,y)\n",
    "array_ = lambda d: np.array([d[k] for k in d.keys() if k != 'SK'])\n",
    "data = [array_(candidates_psChi2), array_(candidates_JS),\n",
    "        array_(candidates_IOU), array_(candidates_EM)]\n",
    "stat_full_names = [r'Prob.Symm.$\\chi^2$', 'Jensen-Shannon', 'Ruzicka.IOU', 'Wasserstein.EM']\n",
    "stat_abbrevs = [x.strip('d()') for x in stat_names]\n",
    "pairs = [(0,[1,2,3]), (1,[2,3]), (2,[3])]\n",
    "for row, columns in pairs:\n",
    "    for col in columns:\n",
    "        plt.subplot(3,3,row*3+col)\n",
    "        plt.plot([min(data[col]),max(data[col])], [min(data[row]),max(data[row])], c=\"#cccccc\")\n",
    "        plt.scatter(data[col], data[row], s=10)\n",
    "        if col == columns[0]:\n",
    "            plt.xlabel(stat_abbrevs[col], fontsize=8)\n",
    "            plt.ylabel(stat_abbrevs[row], fontsize=8)\n",
    "        else:\n",
    "            plt.xticks([])\n",
    "        pearson = np.corrcoef(data[row], data[col])[0,1]\n",
    "        plt.text(min(data[col]), 0.6*max(data[row]), r\"$\\rho$={}\".format('%.4f' % pearson))\n",
    "        plt.title(f\"{stat_full_names[row]} vs {stat_full_names[col]}\", fontsize=9)\n",
    "plt.savefig(hdist_crossplots_file, bbox_inches='tight', pad_inches=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw histograms for selected models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw histograms for selected models\n",
    "nsim = min(32, 2**int(np.floor(np.log2(max_simul))))\n",
    "selected = ['SK_nst', 'OK_nst', 'GP(G)_nst', 'GP(L)_nst',\n",
    "            f\"SK_SGS_from_{nsim}\", f\"OK_SGS_from_{nsim}\",\n",
    "            f\"GP_CRF_from_{nsim}\", f\"GP_SGS_from_{nsim}\"]\n",
    "fig = plt.figure(figsize=(12,15))\n",
    "w = np.median(np.diff(bins_representation))\n",
    "for i, k in enumerate(selected):\n",
    "    p = candidates_hist[k]\n",
    "    q = candidates_hist['GroundTruth']\n",
    "    plt.subplot(4,2,i+1)\n",
    "    plt.bar(bins_representation, p, edgecolor=None, width=w, label=k)\n",
    "    plt.bar(bins_representation, q, facecolor=None, fill=False, edgecolor='k', width=w, label='GroundTruth')\n",
    "    plt.legend(fontsize=9, loc='upper left')\n",
    "    if i >= len(selected)-2:\n",
    "        plt.xlabel('grade')\n",
    "    plt.title(f\"{k} probability mass function\", fontsize=10)\n",
    "plt.savefig(histogram_file, bbox_inches='tight', pad_inches=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category 2: Variograms to measure model fidelity (local accuracy/spatial correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variogram configuration parameters and basic definitions\n",
    "specs = {'variogram:linear_vertical_scale': False}\n",
    "\n",
    "cfg_krige['variogram:use_nugget'] = specs.get('variogram:use_nugget', False)\n",
    "cfg_krige['variogram:max_lag'] = specs.get('variogram:max_lag', 250.0)\n",
    "cfg_krige['variogram:num_lag'] = specs.get('variogram:num_lag', 45)\n",
    "cfg_krige['variogram:required_samples'] = specs.get('variogram:required_samples', 30)\n",
    "vgram = {}\n",
    "variogram_model = cfg_krige.get('kriging:covariance_fn', 'matern')\n",
    "fixed_nu = cfg_krige.get('kriging:matern_smoothness', None)\n",
    "max_range = 2 * cfg_krige['variogram:max_lag']\n",
    "\n",
    "# Option to constrain the `nu` smoothness parameter for Matern.\n",
    "# General constraints = (lower, upper) where for instance\n",
    "# upper = [max_range, max_sill, max_nu, max_nugget]\n",
    "constraints = None\n",
    "if fixed_nu:\n",
    "    constraints = ([0., 0., fixed_nu - 0.0001, 0.],\n",
    "                   [max_range, max_var, fixed_nu + 0.0001, 0.5 * max_var])\n",
    "    if cfg_krige['variogram:use_nugget'] is False:\n",
    "        constraints = tuple([cs[:-1] for cs in constraints])\n",
    "\n",
    "make_variogram = lambda x, y : skg.Variogram(\n",
    "                 coordinates=x,\n",
    "                 values=y,\n",
    "                 estimator='matheron',\n",
    "                 model=variogram_model,\n",
    "                 use_nugget=cfg_krige['variogram:use_nugget'],\n",
    "                 maxlag=max_range,\n",
    "                 n_lags=cfg_krige['variogram:num_lag'],\n",
    "                 fit_bounds=constraints)\n",
    "\n",
    "class VariogramValues:\n",
    "    def __init__(self, b=None, e=None):\n",
    "        self.bins = b\n",
    "        self.experimental = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute variograms for models in the focused group\n",
    "if len(ground_truth) >= cfg_krige['variogram:required_samples']:\n",
    "    print(f\"computing variograms\", end=': ')\n",
    "    # Compute variograms for candidates\n",
    "    for model in candidates_mu.keys():\n",
    "        x = df_domain_infer[['X','Y','Z']].values\n",
    "        y = candidates_mu[model].flatten()\n",
    "        retain = np.where(np.isfinite(y))[0]\n",
    "        vgram[model] = make_variogram(x[retain], y[retain])\n",
    "        print(f\"{model}\", end=', ')\n",
    "\n",
    "    # Compute variograms for ground truth types\n",
    "    for model, x, y in zip(['GroundTruth(blocks)', 'GroundTruth(training bh)'],\n",
    "                           [df_domain_infer[['X','Y','Z']].values, df_domain_bh[['X','Y','Z']].values],\n",
    "                           [ground_truth, df_domain_bh['V'].values]):\n",
    "        retain = np.where(np.isfinite(y))[0]\n",
    "        vgram[model] = make_variogram(x[retain], y[retain])\n",
    "        print(f\"{model}\", end=', ')\n",
    "\n",
    "    # Compute variogram stats for models declared thus far\n",
    "    # - df_v.columns = [\"method\", \"serial\", \"bins\", \"variogram($A)\", \"p25($A)\", \"p50($A)\",\n",
    "    #                    \"p75($A)\", ratios($A), \"p25($B)\", \"p50($B)\", \"p75($B)\", ratios($B)]\n",
    "    #   where \"$A\"=\"GroundTruth(training bh)\", \"$B\"=\"GroundTruth(training bh)\"\n",
    "    #         \"method\" is synonymous with \"model\" (such as \"OK-SGS\")\n",
    "    #         \"serial\" describes the inference period and domain as f\"{mA}:{domain_id}\"\n",
    "    ratios, stats, df_v = compute_variogram_stats(\n",
    "                          vgram,\n",
    "                          serial=f\"{mA}:{domain_id}\",\n",
    "                          references=['GroundTruth(blocks)', 'GroundTruth(training bh)'],\n",
    "                          percentiles=[25,50,75])\n",
    "\n",
    "    # Compute typical variogram from single-shot simulation\n",
    "    x = df_domain_infer[['X','Y','Z']].values\n",
    "    for model, pred in zip(['SK_SGS', 'OK_SGS', 'GP_SGS', 'GP_CRF'],\n",
    "                           [sksgs_vals, oksgs_vals, gpsgs_vals, gpcrf_vals]):\n",
    "        individual_vgram = []\n",
    "        for simul in np.arange(min(pred.shape[1], 16)):\n",
    "            y = pred[:, simul]\n",
    "            retain = np.where(np.isfinite(y))[0]\n",
    "            vario = make_variogram(x[retain], y[retain])\n",
    "            individual_vgram.append(vario.experimental)\n",
    "        key = model + '_single'\n",
    "        vgram[key] = VariogramValues(vario.bins, np.mean(individual_vgram, axis=0))\n",
    "        print(f\"{key}\", end=', ')\n",
    "\n",
    "    # Compute variograms for remaining \"black box\" models\n",
    "    for model, x, y in zip(['RTK_LongRangeModel'],\n",
    "                           [df_domain_infer[['X','Y','Z']].values] * 1,\n",
    "                           [df_domain_infer['cu_50'].values]):\n",
    "        retain = np.where(np.isfinite(y))[0]\n",
    "        vgram[model] = make_variogram(x[retain], y[retain])\n",
    "        print(f\"{model}\", end=', ')\n",
    "\n",
    "    with open(variogram_stats_pfile, 'wb') as hdl:\n",
    "        pickle.dump([ratios, stats, df_v], hdl, protocol=4)\n",
    "else:\n",
    "    models = list(candidates_mu.keys()) + ['GroundTruth(blocks)', 'GroundTruth(training bh)']\n",
    "    vgram_ratios50 = [np.nan] * len(models)\n",
    "    df_v = pd.DataFrame(zip(models, vgram_ratios50), columns=['method','p50(GroundTruth(blocks))'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate variogram plots using a consistent colour scheme\n",
    "# Curves for model <m> consist of\n",
    "#    [\"<m>_SGS_single\"] +                      #R.1\n",
    "#    [\"<m>_SGS_from{t}\" for t in two_powers] + #R.2\n",
    "#    [\"<m>_nst\"] if <m> ! 'GP(L)' else [] +    #R.3\n",
    "#    ['GP(L)_nst] +                            #R.4\n",
    "#    [\"RTK_LongRangeModel\"] +                  #R.5\n",
    "#    [\"GroundTruth(blocks)\", \"GroundTruth(training bh)\"] #R.6\n",
    "two_powers = [int(k.split('_')[-1]) for k in candidates_mu if 'CRF_from' in k]\n",
    "reserved_patterns = ['-<','->','-x','-o','--*','-.p',':P',':s']\n",
    "n = len(two_powers)\n",
    "clut = lambda words: [variograms_colour_lookup(w) for w in words]\n",
    "common = ['lilac', 'gray'] + ['black']*2 #covers R.4-R.6\n",
    "palette = {'SK_SGS': clut(['olive'] + ['green-mid']*n + ['green-dark'] + common),\n",
    "           'OK_SGS': clut(['magenta'] + ['blue-light']*n + ['blue-dark'] + common),\n",
    "           'GP_SGS': clut(['red-dark'] + ['red']*n + common),\n",
    "           'GP_CRF': clut(['orange-dark'] + ['orange']*n + ['pink'] + common)\n",
    "          }\n",
    "\n",
    "if len(ground_truth) >= cfg_krige['variogram:required_samples']:\n",
    "    vgram_peak = max([np.nanmax(v.experimental) for k,v in vgram.items()])\n",
    "    vgram_trough = min([np.nanmin(v.experimental) for k,v in vgram.items()])\n",
    "    linear_vertical_scale = specs.get('variogram:linear_vertical_scale', False)\n",
    "    if not linear_vertical_scale:\n",
    "        vgram_trough = np.maximum(1e-4, vgram_trough)\n",
    "    category_items = OrderedDict()\n",
    "\n",
    "    categories = ['SK_SGS', 'OK_SGS', 'GP_SGS', 'GP_CRF']\n",
    "    base_techniques = ['SK_nst', 'OK_nst', 'GP(L)_nst', 'GP(G)_nst']\n",
    "    for base, cat in zip(base_techniques, categories):\n",
    "        print(f\"Producing variogram plot for {cat}\")\n",
    "        category_items[cat] = [f\"{cat}_single\"] \\\n",
    "                            + [x for x in vgram.keys() if f\"{cat}_from\" in x] \\\n",
    "                            + [base] + (['GP(L)_nst'] if base != 'GP(L)_nst' else []) \\\n",
    "                            + ['RTK_LongRangeModel'] \\\n",
    "                            + ['GroundTruth(blocks)', 'GroundTruth(training bh)']\n",
    "        patterns = ['-^'] + reserved_patterns[:n] \\\n",
    "                 + (['-o'] if base != 'GP' else []) + ['-o','-o','-o','-o','--o']\n",
    "        colors = palette[cat]\n",
    "        plt.figure(figsize=(10,8))\n",
    "        for i, model in enumerate(category_items[cat]):\n",
    "            xe = vgram[model].bins\n",
    "            ye = vgram[model].experimental\n",
    "            if linear_vertical_scale:\n",
    "                plt.plot(xe, ye, patterns[i], color=colors[i], markersize=4, label=model)\n",
    "                legend_position = 'upper left'\n",
    "            else:\n",
    "                plt.semilogy(xe, ye, patterns[i], color=colors[i], markersize=4, label=model)\n",
    "                legend_position = 'lower right'\n",
    "        plt.title(f\"Variograms comparison (models in the {cat} family)\")\n",
    "        plt.ylim([vgram_trough, np.ceil(1000 * vgram_peak) / 1000])\n",
    "        plt.xlabel('Lag [m]')\n",
    "        plt.ylabel('Semivariance')\n",
    "        plt.legend(loc=legend_position)\n",
    "        plt.savefig(variogram_file.replace('@', cat.lower()), bbox_inches='tight', pad_inches=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category 3: Uncertainty-based measures\n",
    "### Evaluate performance of probabilistic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a default p vector with 256 bins\n",
    "# - comprising two linear segments with denser spacing in the upper echelon\n",
    "default_p_values = np.r_[np.linspace(0,0.98,247)[1:-1], np.linspace(0.98,1,12)[:-1]]\n",
    "p_values = specs.get('uncertainty:p_values', default_p_values)\n",
    "\n",
    "if os.path.exists(uncertainty_stats_pfile):\n",
    "    with open(uncertainty_stats_pfile, 'rb') as f:\n",
    "        (candidates_s, candidates_L, candidates_K, candidates_A,\n",
    "         candidates_P, candidates_G, candidates_W, candidates_T,\n",
    "         df_a, df_s) = pickle.load(f)\n",
    "        df_s.rename(columns={'Likelihood': 'Consensus'}, inplace=True)",
    "    stat_names = list(df_s.columns)\n",
    "else:\n",
    "    # Investigate the sensitivity of kappa accuracy measure\n",
    "    candidates_A_slack = OrderedDict()\n",
    "    keys_a = []\n",
    "    for xi in [0, 0.005, 0.01, 0.05, 0.1, 0.25]:\n",
    "        keys_a.append(f\"Accuracy({xi})\")\n",
    "        candidates_A_slack[xi] = OrderedDict()\n",
    "        for k in candidates_mu.keys():\n",
    "            s_scores = compute_model_consensus(mu_0, candidates_mu[k], candidates_sigma[k])[1]\n",
    "            accuracy = compute_distribution_accuracy(p_values, None, s_scores, slack=xi)\n",
    "            candidates_A_slack[xi][k] = accuracy\n",
    "\n",
    "    data_a = OrderedDict()\n",
    "    for k in candidates_mu.keys():\n",
    "        data_a[k] = np.round([d[k] for s, d in candidates_A_slack.items()], 4)\n",
    "    df_a = pd.DataFrame.from_dict(data_a, orient='index', columns=keys_a)\n",
    "\n",
    "df_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consider the sensitivity of the kappa accuracy measure\n",
    "Reflecting on the definition of distribution accuracy, the original definition does seem to be unnecessarily strict. In the `Accuracy(0)` column, the maximum accuracy attained by some models frequently lies below 0.8.\n",
    "- The accuracy score, $A = \\int_{0}^{1} [\\bar{\\kappa}(p)\\ge p]\\,dp$, where $\\bar{\\kappa}(p) = \\frac{1}{m}\\sum_{j=1}^m \\kappa_j(p)$ depends on\n",
    "- $\\kappa_j(p) =\\begin{cases}1 & \\text{if }\\hat{Q}_{(1-p)/2}(j) < Y_j < \\hat{Q}_{(1+p)/2}(j) \\iff p \\ge p_j^{*}\\text{ where }p_j^{*} = 1 - l(\\hat{\\mu_j},\\hat{\\sigma_j}\\mid\\mu_{0,j})\\\\0 & \\text{otherwise}\\\\ \\end{cases}$\n",
    "\n",
    "Very often, $\\bar{\\kappa}(p) \\gtrsim p$ is close to or in the vicinity of $p$. This inequality condition can be relaxed by introducing a slack variable $\\xi$, such that\n",
    "- $A_{\\xi} = \\int_{0}^{1} [\\bar{\\kappa}(p)\\ge (1-\\xi)p]\\,dp$\n",
    "\n",
    "with typical values of $1-\\xi\\in [0.9,1]$ to better reflect the accuracy of distribution in the (Deutsch, 1997) sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(uncertainty_stats_pfile):\n",
    "    # Compute uncertainty-based measures\n",
    "    # - s, L and K denote the signed affinity, local consensus, and mean kappa proportions\n",
    "    # - A, P, G, W, and T denote the accuracy, precision, goodness, width and tightness\n",
    "    #   of the conditional distribution function from a given model\n",
    "    candidates_s = OrderedDict()\n",
    "    candidates_L = OrderedDict()\n",
    "    candidates_K = OrderedDict()\n",
    "    candidates_A = OrderedDict()\n",
    "    candidates_P = OrderedDict()\n",
    "    candidates_G = OrderedDict()\n",
    "    candidates_W = OrderedDict()\n",
    "    candidates_T = OrderedDict()\n",
    "    for k in candidates_mu.keys():\n",
    "        d = compute_performance_statistics(mu_0, candidates_mu[k], candidates_sigma[k], slack=0.05)\n",
    "        candidates_s[k] = d['s_scores']\n",
    "        candidates_L[k] = np.mean(d['consensus'])\n",
    "        candidates_K[k] = d['proportion']\n",
    "        candidates_A[k] = d['accuracy']\n",
    "        candidates_P[k] = d['precision']\n",
    "        candidates_G[k] = d['goodness']\n",
    "        candidates_W[k] = d['width']\n",
    "        candidates_T[k] = d['tightness']\n",
    "\n",
    "    data_precise = OrderedDict()\n",
    "    data_rounded = OrderedDict()\n",
    "    stat_names = ['h(psChi2)', 'h(JS)', 'h(IOU)', 'h(EM)', 'h(rank)',\n",
    "                  'Variogram Ratios', 'Spatial Fidelity', '|s|_L', '|s|_U', 'Consensus',\n",
    "                  'Accuracy(.05)', 'Precision', 'Goodness', 'Tightness']\n",
    "    for k in candidates_mu.keys():\n",
    "        s = np.abs(candidates_s[k])\n",
    "        variogram_ratio = df_v[df_v.method==k]['p50(GroundTruth(blocks))'].values[0]\n",
    "        spatial_fidelity = np.sqrt(1 - np.abs(np.minimum(variogram_ratio, 2) - 1))\n",
    "        record = [candidates_psChi2[k], candidates_JS[k], candidates_IOU[k],\n",
    "                  candidates_EM[k], df_h.loc[k,'rank(overall)'],\n",
    "                  variogram_ratio, spatial_fidelity,\n",
    "                  np.percentile(s,25), np.percentile(s,75), candidates_L[k],\n",
    "                  candidates_A[k], candidates_P[k], candidates_G[k], candidates_T[k]]\n",
    "        data_precise[k] = record\n",
    "        data_rounded[k] = np.round(record, 4)\n",
    "    df_s = pd.DataFrame.from_dict(data_precise, orient='index', columns=stat_names)\n",
    "    df_s.to_csv(analysis_csv)\n",
    "\n",
    "    with open(uncertainty_stats_pfile, 'wb') as hdl:\n",
    "        variables = [candidates_s, candidates_L, candidates_K, candidates_A,\n",
    "                     candidates_P, candidates_G, candidates_W, candidates_T, df_a, df_s]\n",
    "        pickle.dump(variables, hdl, protocol=4)\n",
    "\n",
    "df_s.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphs - Prediction uncertainty accuracy and width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Produce graphs on prediction uncertainty accuracy and interval width\n",
    "p_vals = np.r_[np.linspace(0,1,41)[1:-1], 0.9825, 0.99, 0.997]\n",
    "alpha = np.linspace(0.025, 0.99, 100)\n",
    "n_sigma = norm.ppf(1 - (1 - np.array(alpha))/2, 0, 1)\n",
    "\n",
    "fig = plt.figure(figsize=(12,30))\n",
    "for i, k in enumerate(selected):\n",
    "    s_scores = candidates_s[k]\n",
    "    sigma_hat = candidates_sigma[k]\n",
    "    signal_variance = np.var(ground_truth)\n",
    "    kappa_scores = compute_kappa_bar(s_scores, p_vals)\n",
    "    interval_widths = compute_width_bar(sigma_hat, s_scores, p_vals)\n",
    "    # kappa (accuracy) plots\n",
    "    plt.subplot(8,2,2*i+1)\n",
    "    plt.plot(p_vals, p_vals, '-', color=\"#888888\")\n",
    "    plt.plot(p_vals, kappa_scores, 'k')\n",
    "    plt.ylim([0,1])\n",
    "    plt.text(0.375, 0.25, \"Distribution accuracy: A=%.6f\" % candidates_A[k], fontsize=9)\n",
    "    plt.text(0.375, 0.175, \"Coverage precision: P=%.6f\" % candidates_P[k], fontsize=9)\n",
    "    plt.text(0.375, 0.1, \"Deutsch goodness statistic: G=%.6f\" % candidates_G[k], fontsize=9)\n",
    "    plt.legend([\"expected proportions\", r\"$\\kappa(\\hat{\\mu},\\hat{\\sigma}\\mid\\mu_0$)\"], loc='upper left')\n",
    "    plt.title(f\"Kappa plot - Expected vs estimated proportions for {k}\", fontsize=10)\n",
    "    if i >= len(selected) - 1:\n",
    "        plt.xlabel('p')\n",
    "    # W plots\n",
    "    plt.subplot(8,2,2*i+2)\n",
    "    n_sigma = norm.ppf(1 - (1 - np.array(alpha))/2, 0, 1)\n",
    "    plt.plot(alpha, n_sigma, '-', color=\"#888888\")\n",
    "    plt.plot(p_vals, interval_widths / np.sqrt(signal_variance), 'k')\n",
    "    plt.ylim([0,2])\n",
    "    plt.text(0.0, 1.4, \"Black curve: the lower the better\", fontsize=9, color=\"#888888\")\n",
    "    plt.text(0.25, 0.1, r\"Tightness statistic: T=%.6f [for $\\kappa_j(p)>p]$\" % candidates_T[k], fontsize=9)\n",
    "    plt.legend([\"norminv(1-(1-p)/2)\", r\"Width of prediction interval: $\\bar{W}(p)/\\sigma_Y$\"], loc='upper left')\n",
    "    plt.title(f\"W plot - normalised predict interval width vs p for {k}\", fontsize=10)\n",
    "    if i >= len(selected) - 1:\n",
    "        plt.xlabel('p')    \n",
    "plt.savefig(kappa_w_file, bbox_inches='tight', pad_inches=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "We continue to use the prediction interval width\n",
    "- $\\bar{W}(p) = \\dfrac{1}{m \\bar{\\kappa}(p)}\\sum_{j=1}^{m} \\kappa_j(p)\\left[\\hat{Q}_{(1+p)/2}(j)-\\hat{Q}_{(1-p)/2}(j)\\right]$\n",
    "as defined by (Goovaerts, 2001).\n",
    "\n",
    "However, we incorporate a signal variance normalisation in our tightness definition\n",
    "- $T = \\dfrac{1}{\\sigma_Y} \\int_{0}^{1} \\bar{W}(p) dp$,\n",
    "where $\\sigma_Y$ denotes the standard deviation observed in the validation data.\n",
    "\n",
    "If we write $\\bar{W}'(p) = \\dfrac{\\bar{W}(p)}{\\sigma_Y}$, this allows us to compare the $\\bar{W}'$ curves as a function of $p$ with the inverse normal cdf, which illustrates more clearly the <font color=\"#cc0066\">predictive confidence</font> of a model relative to the variability of the underlying process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation of model estimates $\\mu$ and $\\sigma$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,12))\n",
    "print(\"Predicted mean comparison for main model candidates\")\n",
    "for i, k in enumerate(selected):\n",
    "    cbar_desc = \"mu(Cu)\"\n",
    "    hide_x_axis = False if i >= len(selected)-2 else True\n",
    "    hide_y_axis = False if i%2==0 else True\n",
    "    make_scatter_2d(\n",
    "        df_domain_infer['X'], df_domain_infer['Y'], candidates_mu[k],\n",
    "        min_v, max_v, symbsiz=50, subplotargs=[4,2,i+1], palette='YlOrRd', cbtitle=cbar_desc,\n",
    "        sharex=hide_x_axis, sharey=hide_y_axis, symbol='s',\n",
    "        graphtitle=f\"{k} predicted mean\")\n",
    "plt.savefig(spatial_mean_file.replace('@', 'for_candidates'), bbox_inches='tight', pad_inches=0.05)\n",
    "\n",
    "print(\"Predicted mean as a function of number of simulations\")\n",
    "for cat in ['OK_SGS', 'GP_SGS', 'GP_CRF']:\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    for i, m in enumerate(two_powers):\n",
    "        make_scatter_2d(\n",
    "            df_domain_infer['X'], df_domain_infer['Y'], candidates_mu[f\"{cat}_from_{m}\"],\n",
    "            min_v, max_v, symbsiz=50, subplotargs=[4,2,i+1], palette='YlOrRd', cbtitle=\"mu(Cu)\",\n",
    "            sharex=True, symbol='s', graphtitle=f\"{cat} predicted Cu mean(from {m} runs)\")\n",
    "    plt.savefig(spatial_mean_file.replace('@', f\"{cat}_convergence\"), bbox_inches='tight', pad_inches=0.05)\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "print(\"Predicted standard deviation comparison for main model candidates\")\n",
    "for i, k in enumerate([m for m in selected if m not in ['SK','OK']]): #code based on notebook 2D.7\n",
    "    cbar_desc = \"sigma(Cu)\"\n",
    "    hide_x_axis = False if i >= len(selected)-2 else True\n",
    "    hide_y_axis = False if i%2==0 else True\n",
    "    ymin = np.percentile(candidates_sigma[k], 5) if k in ['SK','OK'] else min_vsd\n",
    "    ymax = np.percentile(candidates_sigma[k], 95) if k in ['SK','OK'] else max_vsd\n",
    "    make_scatter_2d(\n",
    "        df_domain_infer['X'], df_domain_infer['Y'], candidates_sigma[k],\n",
    "        ymin, ymax, symbsiz=50, subplotargs=[4,2,i+1], palette='Blues', cbtitle=cbar_desc,\n",
    "        sharex=hide_x_axis, sharey=hide_y_axis, symbol='s',\n",
    "        graphtitle=f\"{k} predicted stdev\")\n",
    "plt.savefig(spatial_stdev_file.replace('@', 'for_candidates'), bbox_inches='tight', pad_inches=0.05)\n",
    "\n",
    "print(\"Predicted standard deviation as a function of number of simulations\")\n",
    "for cat in ['OK_SGS', 'GP_SGS', 'GP_CRF']:\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    for i, m in enumerate(two_powers):\n",
    "        make_scatter_2d(\n",
    "            df_domain_infer['X'], df_domain_infer['Y'], candidates_sigma[f\"{cat}_from_{m}\"],\n",
    "            min_vsd, max_vsd, symbsiz=50, subplotargs=[4,2,i+1], palette='Blues', cbtitle=\"sigma(Cu)\",\n",
    "            sharex=True, symbol='s', graphtitle=f\"{cat} predicted Cu stdev(from {m} runs)\")\n",
    "    plt.savefig(spatial_stdev_file.replace('@', f\"{cat}_convergence\"), bbox_inches='tight', pad_inches=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Synchronicity: $s(\\hat{\\mu},\\hat{\\sigma}\\mid\\mu_0)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cmapBlRd = create_inverted_colormap(gamma=2.5)\n",
    "fig = plt.figure(figsize=(12,15))\n",
    "print('Using s score to illustrate relative distortion')\n",
    "print('Visual interpretation: Map of questionable errors or least probable predictions')\n",
    "print(' - Dark: bad, light: good')\n",
    "print(' - Red: under-estimated, blue: over-estimated')\n",
    "displayed_models = [m for m in selected if m!='OK' and m!='SK']\n",
    "for i, k in enumerate(displayed_models):\n",
    "    cbar_desc = \"red=under-estimated, 0=worst\" if i%2==1 else \"quality\"\n",
    "    hide_x_axis = False if i >= len(displayed_models)-2 else True\n",
    "    hide_y_axis = False if i%2==0 else True\n",
    "    make_scatter_2d(\n",
    "        df_domain_infer['X'], df_domain_infer['Y'], candidates_s[k],\n",
    "        -1, +1, symbsiz=50, subplotargs=[4,2,i+1], palette=cmapBlRd, cbtitle=cbar_desc,\n",
    "        sharex=hide_x_axis, sharey=hide_y_axis, symbol='s',\n",
    "        graphtitle=r\"Distortion Map, $s(\\hat{\\mu},\\hat{\\sigma}\\mid\\mu_0)$ for \" + f\"{k}\")\n",
    "plt.savefig(signed_distortion_file, bbox_inches='tight', pad_inches=0.05)\n",
    "\n",
    "cmapBl = create_inverted_colormap(gamma=0.7, monochrome=True)\n",
    "fig = plt.figure(figsize=(12,15))\n",
    "print('Using consensus score to illustrate quality of probabilistic prediction')\n",
    "for i, k in enumerate(displayed_models):\n",
    "    hide_x_axis = False if i >= len(displayed_models)-2 else True\n",
    "    hide_y_axis = False if i%2==0 else True\n",
    "    make_scatter_2d(\n",
    "        df_domain_infer['X'], df_domain_infer['Y'], np.abs(candidates_s[k]),\n",
    "    0, +1, symbsiz=50, subplotargs=[4,2,i+1], palette=cmapBl, cbtitle=\"consensus\",\n",
    "    sharex=hide_x_axis, sharey=hide_y_axis, symbol='s',\n",
    "    graphtitle=r\"Local consensus $l(\\hat{\\mu},\\hat{\\sigma}\\mid\\mu_0)$ for \" + f\"{k}\")\n",
    "plt.savefig(local_consensus_file, bbox_inches='tight', pad_inches=0.05)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
